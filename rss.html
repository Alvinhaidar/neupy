<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>NeuPy</title>
        <link>http://neupy.com/</link>
        <description>Neural Networks in Python</description>
        <language>en-us</language>
        <pubDate>Sat, 17 Dec 2016 00:00:00 +0100</pubDate>
        
        <item>
            <link>http://neupy.com/2016/12/17/hyperparameter_optimization_for_neural_networks.html</link>
            <guid>http://neupy.com/2016/12/17/hyperparameter_optimization_for_neural_networks.html</guid>
            <title><![CDATA[Hyperparameter optimization for Neural Networks]]></title>
            <description><![CDATA[<h1><a class="toc-backref" href="#id13">Hyperparameter optimization for Neural Networks</a></h1>
<div class="contents topic" id="contents">
<p class="topic-title first">Contents</p>
<ul class="simple">
<li><a class="reference internal" href="http://neupy.com/2016/12/17/hyperparameter_optimization_for_neural_networks.html#hyperparameter-optimization-for-neural-networks" id="id13">Hyperparameter optimization for Neural Networks</a><ul>
<li><a class="reference internal" href="http://neupy.com/2016/12/17/hyperparameter_optimization_for_neural_networks.html#introduction" id="id14">Introduction</a></li>
<li><a class="reference internal" href="http://neupy.com/2016/12/17/hyperparameter_optimization_for_neural_networks.html#hyperparameter-optimization" id="id15">Hyperparameter optimization</a></li>
<li><a class="reference internal" href="http://neupy.com/2016/12/17/hyperparameter_optimization_for_neural_networks.html#grid-search" id="id16">Grid Search</a></li>
<li><a class="reference internal" href="http://neupy.com/2016/12/17/hyperparameter_optimization_for_neural_networks.html#random-search" id="id17">Random Search</a></li>
<li><a class="reference internal" href="http://neupy.com/2016/12/17/hyperparameter_optimization_for_neural_networks.html#hand-tuning" id="id18">Hand-tuning</a></li>
<li><a class="reference internal" href="http://neupy.com/2016/12/17/hyperparameter_optimization_for_neural_networks.html#bayesian-optimization" id="id19">Bayesian Optimization</a><ul>
<li><a class="reference internal" href="http://neupy.com/2016/12/17/hyperparameter_optimization_for_neural_networks.html#gaussian-process" id="id20">Gaussian Process</a></li>
<li><a class="reference internal" href="http://neupy.com/2016/12/17/hyperparameter_optimization_for_neural_networks.html#acquisition-function" id="id21">Acquisition Function</a></li>
<li><a class="reference internal" href="http://neupy.com/2016/12/17/hyperparameter_optimization_for_neural_networks.html#find-number-of-hidden-units" id="id22">Find number of hidden units</a></li>
<li><a class="reference internal" href="http://neupy.com/2016/12/17/hyperparameter_optimization_for_neural_networks.html#disadvantages-of-gp-with-ei" id="id23">Disadvantages of GP with EI</a></li>
</ul>
</li>
<li><a class="reference internal" href="http://neupy.com/2016/12/17/hyperparameter_optimization_for_neural_networks.html#tree-structured-parzen-estimators-tpe" id="id24">Tree-structured Parzen Estimators (TPE)</a><ul>
<li><a class="reference internal" href="http://neupy.com/2016/12/17/hyperparameter_optimization_for_neural_networks.html#overview" id="id25">Overview</a></li>
<li><a class="reference internal" href="http://neupy.com/2016/12/17/hyperparameter_optimization_for_neural_networks.html#hyperparameter-optimization-for-mnist-dataset" id="id26">Hyperparameter optimization for MNIST dataset</a></li>
<li><a class="reference internal" href="http://neupy.com/2016/12/17/hyperparameter_optimization_for_neural_networks.html#disadvantages-of-tpe" id="id27">Disadvantages of TPE</a></li>
</ul>
</li>
<li><a class="reference internal" href="http://neupy.com/2016/12/17/hyperparameter_optimization_for_neural_networks.html#summary" id="id28">Summary</a></li>
<li><a class="reference internal" href="http://neupy.com/2016/12/17/hyperparameter_optimization_for_neural_networks.html#source-code" id="id29">Source Code</a></li>
<li><a class="reference internal" href="http://neupy.com/2016/12/17/hyperparameter_optimization_for_neural_networks.html#references" id="id30">References</a></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="introduction">
<h2><a class="toc-backref" href="#id14">Introduction</a></h2>
<p>Sometimes it can be difficult to choose a correct architecture for Neural Networks. Usually, this process requires a lot of experience because networks include many parameters. Let’s check some of the most important parameters that we can optimize for the neural network:</p>
<ul class="simple">
<li>Number of layers</li>
<li>Different parameters for each layer (number of hidden units, filter size for convolutional layer and so on)</li>
<li>Type of activation functions</li>
<li>Parameter initialization method</li>
<li>Learning rate</li>
<li>Loss function</li>
</ul>
<p>Even though the list of parameters in not even close to being complete, it’s still impressive how many parameters influences network’s accuracy.</p>
</div>
<div class="section" id="hyperparameter-optimization">
<h2><a class="toc-backref" href="#id15">Hyperparameter optimization</a></h2>
<p>In this article, I would like to show a few different hyperparameter selection methods.</p>
<ul class="simple">
<li>Grid Search</li>
<li>Random Search</li>
<li>Hand-tuning</li>
<li>Gaussian Process with Expected Improvement</li>
<li>Tree-structured Parzen Estimators (TPE)</li>
</ul>
</div>
<div class="section" id="grid-search">
<h2><a class="toc-backref" href="#id16">Grid Search</a></h2>
<p>The simplest algorithms that you can use for hyperparameter optimization is a Grid Search. The idea is simple and straightforward. You just need to define a set of parameter values, train model for all possible parameter combinations and select the best one. This method is a good choice only when model can train quickly, which is not the case for typical neural networks.</p>
<p>Imagine that we need to optimize 5 parameters. Let’s assume, for simplicity, that we want to try 10 different values per each parameter. Therefore, we need to make 100,000 (<span class="math">\(10 ^ 5\)</span>) evaluations. Assuming that network trains 10 minutes on average we will have finished hyperparameter tuning in almost 2 years. Seems crazy, right? Typically, network trains much longer and we need to tune more hyperparameters, which means that it can take forever to run grid search for typical neural network. The better solution is random search.</p>
</div>
<div class="section" id="random-search">
<h2><a class="toc-backref" href="#id17">Random Search</a></h2>
<p>The idea is similar to Grid Search, but instead of trying all possible combinations we will just use randomly selected subset of the parameters. Instead of trying to check 100,000 samples we can check only 1,000 of parameters. Now it should take a week to run hyperparameter optimization instead of 2 years.</p>
<p>Let’s sample 100 two-dimensional data points from a uniform distribution.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="http://neupy.com/_images/100-uniform-data-points.png"><img alt="Randomly generated 100 data points" src="http://neupy.com/_images/100-uniform-data-points.png" style="width: 100%;"/></a>
</div>
<p>In case if there are not enough data points, random sampling doesn’t fully covers parameter space. It can be seen in the figure above because there are some regions that don’t have data points. In addition, it samples some points very close to each other which are redundant for our purposes. We can solve this problem with <a class="reference external" href="https://en.wikipedia.org/wiki/Low-discrepancy_sequence">Low-discrepancy sequences</a> (also called quasi-random sequences).</p>
<p>There are many different techniques for quasi-random sequences:</p>
<ul class="simple">
<li>Sobol sequence</li>
<li>Hammersley set</li>
<li>Halton sequence</li>
<li>Poisson disk sampling</li>
</ul>
<p>Let’s compare some of the mentioned methods with previously random sampled data points.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="http://neupy.com/_images/100-random-points.png"><img alt="Randomly generated 100 data points" src="http://neupy.com/_images/100-random-points.png" style="width: 100%;"/></a>
</div>
<p>As we can see now sampled points spread out through the parameter space more uniformly. One disadvantage of these methods is that not all of them can provide you good results for the higher dimensions. For instance, Halton sequence and Hammersley set do not work well for dimension bigger than 10 <a class="footnote-reference" href="#id11" id="id1">[7]</a>.</p>
<p>Even though we improved hyperparameter optimization algorithm it still is not suitable for large neural networks.</p>
<p>But before we move on to more complicated methods I want to focus on parameter hand-tuning.</p>
</div>
<div class="section" id="hand-tuning">
<h2><a class="toc-backref" href="#id18">Hand-tuning</a></h2>
<p>Let’s start with an example. Imagine that we want to select the best number of units in the hidden layer (we set up just one hyperparameter for simplicity). The simplest thing is to try different values and select the best one. Let’s say we set up 10 units for the hidden layer and train the network. After the training, we check the accuracy for the validation dataset and it turns out that we classified 65% of the samples correctly.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="http://neupy.com/_images/hid-units-vs-accuracy-iter1.png"><img alt="Hidden units vs Accuracy, Iteration #1" src="http://neupy.com/_images/hid-units-vs-accuracy-iter1.png" style="width: 100%;"/></a>
</div>
<p>The accuracy is low, so it’s intuitive to think that we need more units in a hidden layer. Let’s increase the number of units and check the improvement. But, by how many should we increase the number of units? Will small changes make a significant effect on the prediction accuracy? Would it be a good step to set up a number of hidden units equal to 12? Probably not. So let’s go further and explore parameters from the next order of magnitude. We can set up a number of hidden units equal to 100.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="http://neupy.com/_images/hid-units-vs-accuracy-iter2.png"><img alt="Hidden units vs Accuracy, Iteration #2" src="http://neupy.com/_images/hid-units-vs-accuracy-iter2.png" style="width: 100%;"/></a>
</div>
<p>For the 100 hidden units, we got prediction accuracy equal to 82% which is a great improvement compared to 65%. Two points in the figure above show us that by increasing number of hidden units we increase the accuracy. We can proceed using the same strategy and train network with 200 hidden units.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="http://neupy.com/_images/hid-units-vs-accuracy-iter3.png"><img alt="Hidden units vs Accuracy, Iteration #3" src="http://neupy.com/_images/hid-units-vs-accuracy-iter3.png" style="width: 100%;"/></a>
</div>
<p>After the third iteration, our prediction accuracy is 84%. We’ve increased the number of units by a factor of two and got only 2% of improvement.</p>
<p>We can keep going, but I think judging by this example it is clear that human can select parameters better than Grid search or Random search algorithms. The main reason why is that we are able to learn from our previous mistakes. After each iteration, we memorize and analyze our previous results. This information gives us a much better way for selection of the next set of parameters. And even more than that. The more you work with neural networks the better intuition you develop for what and when to use.</p>
<p>Nevertheless, let’s get back to our optimization problem. How can we automate the process described above? One way of doing this is to apply a Bayesian Optimization.</p>
</div>
<div class="section" id="bayesian-optimization">
<h2><a class="toc-backref" href="#id19">Bayesian Optimization</a></h2>
<p>Bayesian optimization is a derivative-free optimization method. There are a few different algorithm for this type of optimization, but I was specifically interested in Gaussian Process with Acquisition Function. For some people it can resemble the method that we’ve described above in the Hand-tuning section. Gaussian Process uses a set of previously evaluated parameters and resulting accuracy to make an assumption about unobserved parameters. Acquisition Function using this information suggest the next set of parameters.</p>
<div class="section" id="gaussian-process">
<h3><a class="toc-backref" href="#id20">Gaussian Process</a></h3>
<p>The idea behind Gaussian Process is that for every input <span class="math">\(x\)</span> we have output <span class="math">\(y = f(x)\)</span>, where <span class="math">\(f\)</span> is a stochastic function. This function samples output from a gaussian distribution. Also, we can say that each input <span class="math">\(x\)</span> has associated gaussian distribution. Which means that for each input <span class="math">\(x\)</span> gaussian process has defined mean  <span class="math">\(\mu\)</span> and standard deviation <span class="math">\(\sigma\)</span> for some gaussian distribution.</p>
<p>Gaussian Process is a generalization of <a class="reference external" href="https://en.wikipedia.org/wiki/Multivariate_normal_distribution">Multivariate Gaussian Distribution</a>. Multivariate Gaussian Distribution is defined by mean vector and covariance matrix, while Gaussian Process is defined by mean function and covariance function. Basically, a function is an infinite vector. Also, we can say that Multivariate Gaussian Distribution is a Gaussian Process for the functions with a discrete number of possible inputs.</p>
<p>I always like to have some picture that shows me a visual description of an algorithm. One of such visualizations of the Gaussian Process I found in the Introduction to Gaussian Process slides <a class="footnote-reference" href="#id7" id="id2">[3]</a>.</p>
<p>Let’s check some Multivariate Gaussian Distribution defined by mean vector <span class="math">\(\mu\)</span></p>
<div class="math">
\[\begin{split}\begin{align*}
    \mu =
    \left[
    \begin{array}{c}
      0.0 &amp; 1.0 \\
    \end{array}
    \right]
\end{align*}\end{split}\]</div>
<p>and covariance matrix <span class="math">\(\Sigma\)</span></p>
<div class="math">
\[\begin{split}\begin{align*}
    \Sigma =
    \left[
    \begin{array}{c}
      1.0 &amp; 0.7 \\
      0.7 &amp; 2.5 \\
    \end{array}
    \right]
\end{align*}\end{split}\]</div>
<p>We can sample 100 points from this distribution and make a scatter plot.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="http://neupy.com/_images/mulvar-gauss-dist-example.png"><img alt="Multivariate Gaussian Distribution Example" src="http://neupy.com/_images/mulvar-gauss-dist-example.png" style="width: 100%;"/></a>
</div>
<p>Another way to visualize these samples might be <a class="reference external" href="https://en.wikipedia.org/wiki/Parallel_coordinates">Parallel Coordinates</a>.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="http://neupy.com/_images/mulvar-gauss-dist-parallel-coords.png"><img alt="Multivariate Gaussian Distribution in Parallel Coordinates Example" src="http://neupy.com/_images/mulvar-gauss-dist-parallel-coords.png" style="width: 100%;"/></a>
</div>
<p>You should understand that lines that connect points are just an imaginary relations between each coordinate. There is nothing in between Random variable #1 and Random variable #2.</p>
<p>An interesting thing happens when we increase the number of samples.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="http://neupy.com/_images/mulvar-gauss-dist-parallel-coords-many-samples.png"><img alt="Multivariate Gaussian Distribution in Parallel Coordinates Example with 3000 samples" src="http://neupy.com/_images/mulvar-gauss-dist-parallel-coords-many-samples.png" style="width: 100%;"/></a>
</div>
<p>Now we can see that lines form a smooth shape. This shape defines a correlation between two random variables. If it’s very narrow in the middle then there is a negative correlation between two random variables.</p>
<p>With scatter plot we are limited to numbers of dimensions that we can visualize, but with Parallel Coordinates we can add more dimensions. Let’s define new Multivariate Gaussian Distribution using 5 random variables.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="http://neupy.com/_images/mulvar-gauss-dist-parallel-coords-5d.png"><img alt="Multivariate Gaussian Distribution in Parallel Coordinates for multiple dimensions" src="http://neupy.com/_images/mulvar-gauss-dist-parallel-coords-5d.png" style="width: 100%;"/></a>
</div>
<p>With more variables, it looks more like a function. We can increase the number of dimensions and still be able to visualize Multivariate Gaussian Distribution. The more dimensions we add the more it looks like a set of functions sampled from the Gaussian Process. But in case of Gaussian Process number of dimensions should be infinite.</p>
<p>Let’s get data from the Hand-tuning section (the one where with 10 hidden units we got 65% of accuracy). Using this data we can train Gaussian Process and predict mean and standard deviation for each point <span class="math">\(x\)</span>.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="http://neupy.com/_images/gaussian-process-example.png"><img alt="Gaussian Process regression example for second and third iterations" src="http://neupy.com/_images/gaussian-process-example.png" style="width: 100%;"/></a>
</div>
<p>The blue region defines 95% confidence interval for each point <span class="math">\(x\)</span>. It’s easy to see that the further we go from the observed samples the wider confidence interval becomes which is a logical conclusion. The opposite is true as well. Very similar to the logic that a person uses to select next set of parameters.</p>
<p>From the plot, it looks like observed data points doesn’t have any variance. In fact, the variance is not zero, it’s just really tiny. That’s because our previous Gaussian Process configuration is expecting that our prediction was obtained from a deterministic function which is not true for most neural networks. To fix it we can change the parameter for the Gaussian Process that defines the amount of noise in observed variables. This trick will not only give us a prediction that is less certain but also a mean of the number of hidden units that won’t go through the observed data points.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="http://neupy.com/_images/gaussian-process-noise-example.png"><img alt="Gaussian Process regression example with noise for second and third iterations" src="http://neupy.com/_images/gaussian-process-noise-example.png" style="width: 100%;"/></a>
</div>
</div>
<div class="section" id="acquisition-function">
<h3><a class="toc-backref" href="#id21">Acquisition Function</a></h3>
<p>Acquisition Function defines the set of parameter for our next step. There are many different functions <a class="footnote-reference" href="#id5" id="id3">[1]</a> that can help us calculate the best value for the next step. One of the most common is Expected Improvement. There are two ways to compute it. In case if we are trying to find minimum we can use this formula.</p>
<div class="math">
\[g_{min}(x) = max(0, y_{min} - y_{lowest\ expected})\]</div>
<p>where <span class="math">\(y_{min}\)</span> is the minimum observed value <span class="math">\(y\)</span> and <span class="math">\(y_{lowest\ expected}\)</span> lowest possible value from the confidence interval associated with each possible value <span class="math">\(x\)</span>.</p>
<p>In our case, we are trying to find the maximum value. With the small modifications, we can change last formula in the way that will identify Expected Improvement for the maximum value.</p>
<div class="math">
\[g_{max}(x) = max(0, y_{highest\ expected} - y_{max})\]</div>
<p>where <span class="math">\(y_{max}\)</span> is the maximum observed value and <span class="math">\(y_{highest\ expected}\)</span> highest possible value from the confidence interval associated with each possible value <span class="math">\(x\)</span>.</p>
<p>Here is an output for each point <span class="math">\(x\)</span> for the Expected Improvement function.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="http://neupy.com/_images/expected-improvement-example.png"><img alt="Expected Improvement example" src="http://neupy.com/_images/expected-improvement-example.png" style="width: 100%;"/></a>
</div>
</div>
<div class="section" id="find-number-of-hidden-units">
<h3><a class="toc-backref" href="#id22">Find number of hidden units</a></h3>
<p>Let’s try to build a hyperparameter optimizer based on Gaussian Process regression and Expected Improvement function. We will continue work with the previous problem where we tried to find the best number of hidden units. But for this time we will try to create a network for digit classification tasks.</p>
<p>Let’s define a function that trains the neural network and return prediction error.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="kn">from</span> <span class="nn">neupy</span> <span class="kn">import</span> <span class="n">algorithms</span><span class="p">,</span> <span class="n">layers</span>

<span class="k">def</span> <span class="nf">train_network</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">,</span> <span class="n">x_train</span><span class="p">,</span> <span class="n">x_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span><span class="p">):</span>
    <span class="n">network</span> <span class="o">=</span> <span class="n">algorithms</span><span class="o">.</span><span class="n">Momentum</span><span class="p">(</span>
        <span class="p">[</span>
            <span class="n">layers</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="mi">64</span><span class="p">),</span>
            <span class="n">layers</span><span class="o">.</span><span class="n">Relu</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">),</span>
            <span class="n">layers</span><span class="o">.</span><span class="n">Softmax</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span>
        <span class="p">],</span>

        <span class="c1"># Randomly shuffle dataset before each</span>
        <span class="c1"># training epoch.</span>
        <span class="n">shuffle_data</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>

        <span class="c1"># Do not show training progress in output</span>
        <span class="n">verbose</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>

        <span class="n">step</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
        <span class="n">error</span><span class="o">=</span><span class="s1">'categorical_crossentropy'</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">network</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>

    <span class="c1"># Calculates categorical cross-entropy error between</span>
    <span class="c1"># predicted value for x_test and y_test value</span>
    <span class="k">return</span> <span class="n">network</span><span class="o">.</span><span class="n">prediction_error</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</pre></div>
</div>
<p>Let’s import digits dataset from scikit-learn.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">neupy</span> <span class="kn">import</span> <span class="n">environment</span>

<span class="n">environment</span><span class="o">.</span><span class="n">reproducible</span><span class="p">()</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_digits</span><span class="p">()</span>
<span class="n">n_samples</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">target</span><span class="o">.</span><span class="n">size</span>
<span class="n">n_classes</span> <span class="o">=</span> <span class="mi">10</span>

<span class="c1"># One-hot encoder</span>
<span class="n">target</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">))</span>
<span class="n">target</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_samples</span><span class="p">),</span> <span class="n">dataset</span><span class="o">.</span><span class="n">target</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>

<span class="n">x_train</span><span class="p">,</span> <span class="n">x_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">dataset</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">train_size</span><span class="o">=</span><span class="mf">0.7</span>
<span class="p">)</span>
</pre></div>
</div>
<p>And for the last step, we need to define parameter selection procedure. First, we need to define a function that performs Gaussian Process regression and returns mean and standard deviation of the prediction for the specified input vector.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.gaussian_process</span> <span class="kn">import</span> <span class="n">GaussianProcess</span>

<span class="k">def</span> <span class="nf">vector_2d</span><span class="p">(</span><span class="n">array</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">array</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">gaussian_process</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">x_test</span><span class="p">):</span>
    <span class="n">x_train</span> <span class="o">=</span> <span class="n">vector_2d</span><span class="p">(</span><span class="n">x_train</span><span class="p">)</span>
    <span class="n">y_train</span> <span class="o">=</span> <span class="n">vector_2d</span><span class="p">(</span><span class="n">y_train</span><span class="p">)</span>
    <span class="n">x_test</span> <span class="o">=</span> <span class="n">vector_2d</span><span class="p">(</span><span class="n">x_test</span><span class="p">)</span>

    <span class="c1"># Train gaussian process</span>
    <span class="n">gp</span> <span class="o">=</span> <span class="n">GaussianProcess</span><span class="p">(</span><span class="n">corr</span><span class="o">=</span><span class="s1">'squared_exponential'</span><span class="p">,</span>
                         <span class="n">theta0</span><span class="o">=</span><span class="mf">1e-1</span><span class="p">,</span> <span class="n">thetaL</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">thetaU</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">gp</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

    <span class="c1"># Get mean and standard deviation for each possible</span>
    <span class="c1"># number of hidden units</span>
    <span class="n">y_mean</span><span class="p">,</span> <span class="n">y_var</span> <span class="o">=</span> <span class="n">gp</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">eval_MSE</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">y_std</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">vector_2d</span><span class="p">(</span><span class="n">y_var</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">y_mean</span><span class="p">,</span> <span class="n">y_std</span>
</pre></div>
</div>
<p>Next, we need to apply to the predicted output Expected Improvement (EI) and find out next optimal step.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="k">def</span> <span class="nf">next_parameter_by_ei</span><span class="p">(</span><span class="n">y_min</span><span class="p">,</span> <span class="n">y_mean</span><span class="p">,</span> <span class="n">y_std</span><span class="p">,</span> <span class="n">x_choices</span><span class="p">):</span>
    <span class="c1"># Calculate expecte improvement from 95% confidence interval</span>
    <span class="n">expected_improvement</span> <span class="o">=</span> <span class="n">y_min</span> <span class="o">-</span> <span class="p">(</span><span class="n">y_mean</span> <span class="o">-</span> <span class="mf">1.96</span> <span class="o">*</span> <span class="n">y_std</span><span class="p">)</span>
    <span class="n">expected_improvement</span><span class="p">[</span><span class="n">expected_improvement</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="n">max_index</span> <span class="o">=</span> <span class="n">expected_improvement</span><span class="o">.</span><span class="n">argmax</span><span class="p">()</span>
    <span class="c1"># Select next choice</span>
    <span class="n">next_parameter</span> <span class="o">=</span> <span class="n">x_choices</span><span class="p">[</span><span class="n">max_index</span><span class="p">]</span>

    <span class="k">return</span> <span class="n">next_parameter</span>
</pre></div>
</div>
<p>And finally, we can override all procedure in one function.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="kn">import</span> <span class="nn">random</span>

<span class="k">def</span> <span class="nf">hyperparam_selection</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">n_hidden_range</span><span class="p">,</span> <span class="n">func_args</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">20</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">func_args</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">func_args</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="n">scores</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">parameters</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="n">min_n_hidden</span><span class="p">,</span> <span class="n">max_n_hidden</span> <span class="o">=</span> <span class="n">n_hidden_range</span>
    <span class="n">n_hidden_choices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">min_n_hidden</span><span class="p">,</span> <span class="n">max_n_hidden</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>

    <span class="c1"># To be able to perform gaussian process we need to</span>
    <span class="c1"># have at least 2 samples.</span>
    <span class="n">n_hidden</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">min_n_hidden</span><span class="p">,</span> <span class="n">max_n_hidden</span><span class="p">)</span>
    <span class="n">score</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">,</span> <span class="o">*</span><span class="n">func_args</span><span class="p">)</span>

    <span class="n">parameters</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">)</span>
    <span class="n">scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">score</span><span class="p">)</span>

    <span class="n">n_hidden</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">min_n_hidden</span><span class="p">,</span> <span class="n">max_n_hidden</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">iteration</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">n_iter</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
        <span class="n">score</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">,</span> <span class="o">*</span><span class="n">func_args</span><span class="p">)</span>

        <span class="n">parameters</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">)</span>
        <span class="n">scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">score</span><span class="p">)</span>

        <span class="n">y_min</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>
        <span class="n">y_mean</span><span class="p">,</span> <span class="n">y_std</span> <span class="o">=</span> <span class="n">gaussian_process</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="n">scores</span><span class="p">,</span>
                                         <span class="n">n_hidden_choices</span><span class="p">)</span>

        <span class="n">n_hidden</span> <span class="o">=</span> <span class="n">next_parameter_by_ei</span><span class="p">(</span><span class="n">y_min</span><span class="p">,</span> <span class="n">y_mean</span><span class="p">,</span> <span class="n">y_std</span><span class="p">,</span>
                                        <span class="n">n_hidden_choices</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">y_min</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">n_hidden</span> <span class="ow">in</span> <span class="n">parameters</span><span class="p">:</span>
            <span class="c1"># Lowest expected improvement value have been achieved</span>
            <span class="k">break</span>

    <span class="n">min_score_index</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">parameters</span><span class="p">[</span><span class="n">min_score_index</span><span class="p">]</span>
</pre></div>
</div>
<p>Now we are able to run a few iterations and find a number of hidden units that gave better results during the training.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="n">best_n_hidden</span> <span class="o">=</span> <span class="n">hyperparam_selection</span><span class="p">(</span>
    <span class="n">train_network</span><span class="p">,</span>
    <span class="n">n_hidden_range</span><span class="o">=</span><span class="p">[</span><span class="mi">50</span><span class="p">,</span> <span class="mi">1000</span><span class="p">],</span>
    <span class="n">func_args</span><span class="o">=</span><span class="p">[</span><span class="n">x_train</span><span class="p">,</span> <span class="n">x_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span><span class="p">],</span>
    <span class="n">n_iter</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<div class="figure align-center">
<a class="reference internal image-reference" href="http://neupy.com/_images/hyperparam-selection-nn-hidden-units.png"><img alt="Select number of hidden units for Neural Network" src="http://neupy.com/_images/hyperparam-selection-nn-hidden-units.png" style="width: 100%;"/></a>
</div>
<p>With small modifications, it’s possible to add an additional functionality to the function that allows optimizing more hyperparameters at once.</p>
</div>
<div class="section" id="disadvantages-of-gp-with-ei">
<h3><a class="toc-backref" href="#id23">Disadvantages of GP with EI</a></h3>
<p>There are a few disadvantages related to the Gaussian Process with Expected Improvement.</p>
<ol class="arabic simple">
<li>It doesn’t work well for categorical variables. In case if neural networks it can be a type of activation function.</li>
<li>GP with EI selects new set of parameters based on the best observation. Neural Network usually involves randomization (like weight initialization and dropout) during the training process which influences a final score. Running neural network with the same parameters can lead to different scores. Which means that our best score can be just lucky output for the specific set of parameters.</li>
<li>It can be difficult to select right hyperparameters for Gaussian Process. Gaussian Process has lots of different kernel types. In addition you can construct more complicated kernels using simple kernels as a building block.</li>
<li>It works slower when number of hyperparameters increases. That’s an issue when you deal with a huge number of parameters.</li>
</ol>
</div>
</div>
<div class="section" id="tree-structured-parzen-estimators-tpe">
<h2><a class="toc-backref" href="#id24">Tree-structured Parzen Estimators (TPE)</a></h2>
<div class="section" id="overview">
<h3><a class="toc-backref" href="#id25">Overview</a></h3>
<p>Tree-structured Parzen Estimators (TPE) fixes disadvantages of the Gaussian Process. Each iteration TPE collects new observation and at the end of the iteration, the algorithm decides which set of parameters it should try next. The main idea is similar, but an algorithm is completely different</p>
<p>At the very beginning, we need to define a prior distribution for out hyperparameters. By default, they can be all uniformly distributed, but it’s possible to associate any hyperparameter with some random unimodal distribution.</p>
<p>For the first few iterations, we need to warn up TPE algorithm. It means that we need to collect some data at first before we can apply TPE. The best and simplest way to do it is just to perform a few iterations of Random Search. A number of iterations for Random Search is a parameter defined by the user for the TPE algorithm.</p>
<p>When we collected some data we can finally apply TPE. The next step is to divide collected observations into two groups. The first group contains observations that gave best scores after evaluation and the second one - all other observations. And the goal is to find a set of parameters that more likely to be in the first group and less likely to be in the second group. The fraction of the best observations is defined by the user as a parameter for the TPE algorithm. Typically, it’s 10-25% of observations.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="http://neupy.com/_images/tpe-observation-groups.png"><img alt="Observation groups for TPE" src="http://neupy.com/_images/tpe-observation-groups.png" style="width: 100%;"/></a>
</div>
<p>As you can see we are no longer rely on the best observation. Instead, we use a distribution of the best observations. The more iterations we use during the Random Search the better distribution we have at the beginning.</p>
<p>The next part of the TPE is to model likelihood probability for each of the two groups. This is the next big difference between Gaussian Process and TPE. For Gaussian Process we’ve modeled posterior probability instead of likelihood probability. Using the likelihood probability from the first group (the one that contains best observations) we sample the bunch of candidates. From the sampled candidates we try to find a candidate that more likely to be in the first group and less likely to be in the second one. The following formula defines Expected Improvement per each candidate.</p>
<div class="math">
\[EI(x) = \frac{l(x)}{g(x)}\]</div>
<p>Where <span class="math">\(l(x)\)</span> is a probability being in the first group and <span class="math">\(g(x)\)</span> is a probability being in the second group.</p>
<p>Here is an example. Let’s say we have predefined distribution for both groups. From the group #1, we sample 6 candidates. And for each, we calculate Expected Improvement. A parameter that has the highest improvement is the one that we will use for the next iteration.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="http://neupy.com/_images/tpe-sampled-candidates.png"><img alt="Candidates sampling for TPE" src="http://neupy.com/_images/tpe-sampled-candidates.png" style="width: 100%;"/></a>
</div>
<div class="figure align-center">
<a class="reference internal image-reference" href="http://neupy.com/_images/tpe-expected-improvement.png"><img alt="Expected improvement for TPE" src="http://neupy.com/_images/tpe-expected-improvement.png" style="width: 100%;"/></a>
</div>
<p>In the example, I’ve used t-distributions, but in TPE distribution models using <a class="reference external" href="https://www.cs.utah.edu/~suyash/Dissertation_html/node11.html">parzen-window density estimators</a>. The main idea is that each sample defines gaussian distribution with specified mean (value of the hyperparameter) and standard deviation. Then all these points stacks together and normalized to assure that output is Probability Density Function (PDF). That’s why <cite>Parzen estimators</cite> appears in the name of the algorithm.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="http://neupy.com/_images/parzen-estimators.png"><img alt="Parzen estimators" src="http://neupy.com/_images/parzen-estimators.png" style="width: 100%;"/></a>
</div>
<p>And the <cite>tree-structured</cite> means that parameter space defines in a form of a tree. Later we will try to find the best number of layers for the network. In our case, we will try to decide whether it’s better to use one or two hidden layers. In case if we use two hidden layers we should define the number of hidden units for the first and second layer independently. If we use one hidden layer we don’t need to define the number of hidden units for the second hidden layer, because it doesn’t exist for the specified set of parameter. Basically, it means that a number of hidden units in the second hidden layer depends on the number of hidden layers. Which means that parameters have tree-structured dependencies.</p>
</div>
<div class="section" id="hyperparameter-optimization-for-mnist-dataset">
<h3><a class="toc-backref" href="#id26">Hyperparameter optimization for MNIST dataset</a></h3>
<p>Let’s make an example. We’re going to use MNIST dataset.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span><span class="p">,</span> <span class="n">preprocessing</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="n">mnist</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">fetch_mldata</span><span class="p">(</span><span class="s1">'MNIST original'</span><span class="p">)</span>

<span class="n">target_scaler</span> <span class="o">=</span> <span class="n">preprocessing</span><span class="o">.</span><span class="n">OneHotEncoder</span><span class="p">()</span>
<span class="n">target</span> <span class="o">=</span> <span class="n">mnist</span><span class="o">.</span><span class="n">target</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">target</span> <span class="o">=</span> <span class="n">target_scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">target</span><span class="p">)</span><span class="o">.</span><span class="n">todense</span><span class="p">()</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">mnist</span><span class="o">.</span><span class="n">data</span> <span class="o">/</span> <span class="mf">255.</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">data</span> <span class="o">-</span> <span class="n">data</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">x_train</span><span class="p">,</span> <span class="n">x_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">data</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span>
    <span class="n">target</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span>
    <span class="n">train_size</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span> <span class="o">/</span> <span class="mf">7.</span><span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
<p>For hyperparameter selection, I’m going to use <a class="reference external" href="https://github.com/hyperopt/hyperopt">hyperopt</a> library. It has implemented TPE algorithm.</p>
<p>The hyperopt library gives the ability to define a prior distribution for each parameter. In the table below you can find information about parameters that we are going to tune.</p>
<table border="1" class="docutils">
<colgroup>
<col width="33%"/>
<col width="33%"/>
<col width="33%"/>
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Parameter name</th>
<th class="head">Distribution</th>
<th class="head">Values</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>Step size</td>
<td>Log-uniform</td>
<td><span class="math">\(x \in [0.01, 0.5]\)</span></td>
</tr>
<tr class="row-odd"><td>Batch size</td>
<td>Log-uniform integer</td>
<td><span class="math">\(x \in [16, 512]\)</span></td>
</tr>
<tr class="row-even"><td>Activation function</td>
<td>Categorical</td>
<td><span class="math">\(x \in \{Relu, PRelu, Elu, Sigmoid, Tanh\}\)</span></td>
</tr>
<tr class="row-odd"><td>Number of hidden layers</td>
<td>Categorical</td>
<td><span class="math">\(x \in \{1, 2\}\)</span></td>
</tr>
<tr class="row-even"><td>Number of units in the first layer</td>
<td>Uniform integer</td>
<td><span class="math">\(x \in [50, 1000]\)</span></td>
</tr>
<tr class="row-odd"><td>Number of units in the second layer (In case if it defined)</td>
<td>Uniform integer</td>
<td><span class="math">\(x \in [50, 1000]\)</span></td>
</tr>
<tr class="row-even"><td>Dropout layer</td>
<td>Uniform</td>
<td><span class="math">\(x \in [0, 0.5]\)</span></td>
</tr>
</tbody>
</table>
<p>Here is one way to define our parameters in hyperopt.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">hyperopt</span> <span class="kn">import</span> <span class="n">hp</span>

<span class="k">def</span> <span class="nf">uniform_int</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">lower</span><span class="p">,</span> <span class="n">upper</span><span class="p">):</span>
    <span class="c1"># `quniform` returns:</span>
    <span class="c1"># round(uniform(low, high) / q) * q</span>
    <span class="k">return</span> <span class="n">hp</span><span class="o">.</span><span class="n">quniform</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">lower</span><span class="p">,</span> <span class="n">upper</span><span class="p">,</span> <span class="n">q</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">loguniform_int</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">lower</span><span class="p">,</span> <span class="n">upper</span><span class="p">):</span>
    <span class="c1"># Do not forget to make a logarithm for the</span>
    <span class="c1"># lower and upper bounds.</span>
    <span class="k">return</span> <span class="n">hp</span><span class="o">.</span><span class="n">qloguniform</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">lower</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">upper</span><span class="p">),</span> <span class="n">q</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">parameter_space</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">'step'</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="s1">'step'</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span>
    <span class="s1">'layers'</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">'layers'</span><span class="p">,</span> <span class="p">[{</span>
        <span class="s1">'n_layers'</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
        <span class="s1">'n_units_layer'</span><span class="p">:</span> <span class="p">[</span>
            <span class="n">uniform_int</span><span class="p">(</span><span class="s1">'n_units_layer_11'</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">500</span><span class="p">),</span>
        <span class="p">],</span>
    <span class="p">},</span> <span class="p">{</span>
        <span class="s1">'n_layers'</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
        <span class="s1">'n_units_layer'</span><span class="p">:</span> <span class="p">[</span>
            <span class="n">uniform_int</span><span class="p">(</span><span class="s1">'n_units_layer_21'</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">500</span><span class="p">),</span>
            <span class="n">uniform_int</span><span class="p">(</span><span class="s1">'n_units_layer_22'</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">500</span><span class="p">),</span>
        <span class="p">],</span>
    <span class="p">}]),</span>
    <span class="s1">'act_func_type'</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">'act_func_type'</span><span class="p">,</span> <span class="p">[</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">Relu</span><span class="p">,</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">PRelu</span><span class="p">,</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">Elu</span><span class="p">,</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">Tanh</span><span class="p">,</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">Sigmoid</span>
    <span class="p">]),</span>

    <span class="s1">'dropout'</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="s1">'dropout'</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span>
    <span class="s1">'batch_size'</span><span class="p">:</span> <span class="n">loguniform_int</span><span class="p">(</span><span class="s1">'batch_size'</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">512</span><span class="p">),</span>
<span class="p">}</span>
</pre></div>
</div>
<p>I won’t get into details. I think that definitions are pretty clear from the code. In case if you want to learn more about hyperopt parameter space initialization you can check <a class="reference external" href="https://github.com/hyperopt/hyperopt/wiki/FMin#21-parameter-expressions">this link</a>.</p>
<p>Next we need to construct a function that we want to minimize. In our case function should train network using training dataset and return cross entropy error for validation dataset.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="kn">from</span> <span class="nn">pprint</span> <span class="kn">import</span> <span class="n">pprint</span>

<span class="k">def</span> <span class="nf">train_network</span><span class="p">(</span><span class="n">parameters</span><span class="p">):</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">"Parameters:"</span><span class="p">)</span>
    <span class="n">pprint</span><span class="p">(</span><span class="n">parameters</span><span class="p">)</span>
    <span class="k">print</span><span class="p">()</span>
</pre></div>
</div>
<p>First of all, in the training function, we need to extract our parameter.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="n">step</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s1">'step'</span><span class="p">]</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">parameters</span><span class="p">[</span><span class="s1">'batch_size'</span><span class="p">])</span>
<span class="n">proba</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s1">'dropout'</span><span class="p">]</span>
<span class="n">activation_layer</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s1">'act_func_type'</span><span class="p">]</span>
<span class="n">layer_sizes</span> <span class="o">=</span> <span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">parameters</span><span class="p">[</span><span class="s1">'layers'</span><span class="p">][</span><span class="s1">'n_units_layer'</span><span class="p">]]</span>
</pre></div>
</div>
<p>Note that some of the parameters I converted to the integer. The problem is that hyperopt returns float types and we need to convert them.</p>
<p>Next, we need to construct network based on the presented information. In our case, we use only one or two hidden layers, but it can be any arbitrary number of layers.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="kn">from</span> <span class="nn">neupy</span> <span class="kn">import</span> <span class="n">layers</span>

<span class="n">network</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="mi">784</span><span class="p">)</span>

<span class="k">for</span> <span class="n">layer_size</span> <span class="ow">in</span> <span class="n">layer_sizes</span><span class="p">:</span>
    <span class="n">network</span> <span class="o">=</span> <span class="n">network</span> <span class="o">&gt;</span> <span class="n">activation_layer</span><span class="p">(</span><span class="n">layer_size</span><span class="p">)</span>

<span class="n">network</span> <span class="o">=</span> <span class="n">network</span> <span class="o">&gt;</span> <span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">proba</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">layers</span><span class="o">.</span><span class="n">Softmax</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
<p>To learn more about layers in NeuPy you should check <a class="reference internal" href="http://neupy.com/docs/layers/basics.html#layers-basics"><span>documentation</span></a>.</p>
<p>After that, we can define training algorithm for the network.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="kn">from</span> <span class="nn">neupy</span> <span class="kn">import</span> <span class="n">algorithms</span>
<span class="kn">from</span> <span class="nn">neupy.exceptions</span> <span class="kn">import</span> <span class="n">StopTraining</span>

<span class="k">def</span> <span class="nf">on_epoch_end</span><span class="p">(</span><span class="n">network</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">network</span><span class="o">.</span><span class="n">errors</span><span class="o">.</span><span class="n">last</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">10</span><span class="p">:</span>
        <span class="k">raise</span> <span class="n">StopTraining</span><span class="p">(</span><span class="s2">"Training was interrupted. Error is to high."</span><span class="p">)</span>

<span class="n">mnet</span> <span class="o">=</span> <span class="n">algorithms</span><span class="o">.</span><span class="n">RMSProp</span><span class="p">(</span>
    <span class="n">network</span><span class="p">,</span>

    <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
    <span class="n">step</span><span class="o">=</span><span class="n">step</span><span class="p">,</span>

    <span class="n">error</span><span class="o">=</span><span class="s1">'categorical_crossentropy'</span><span class="p">,</span>
    <span class="n">shuffle_data</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>

    <span class="n">epoch_end_signal</span><span class="o">=</span><span class="n">on_epoch_end</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<p>All settings should be clear from the code. You can check <a class="reference internal" href="http://neupy.com/apidocs/neupy.algorithms.gd.rmsprop.html#neupy.algorithms.gd.rmsprop.RMSProp" title="neupy.algorithms.gd.rmsprop.RMSProp"><span class="xref py py-class docutils literal"><span class="pre">RMSProp</span></span></a> documentation to find more information about its input parameters. In addition, I’ve added a simple rule that interrupts training when the error is too high. This is an example of a simple rule that can be changed.</p>
<p>Now we can train our network.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="n">mnet</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
</pre></div>
</div>
<p>And at the end of the function, we can check some information about the training progress.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="n">score</span> <span class="o">=</span> <span class="n">mnet</span><span class="o">.</span><span class="n">prediction_error</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>

<span class="n">y_predicted</span> <span class="o">=</span> <span class="n">mnet</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_test</span><span class="p">)</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">y_predicted</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s2">"Final score: {}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">score</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s2">"Accuracy: {:.2%}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">accuracy</span><span class="p">))</span>

<span class="k">return</span> <span class="n">score</span>
</pre></div>
</div>
<p>You can see that I’ve used two evaluation metrics. First one is cross-entropy. NeuPy uses it as a validation error function when we call the <span class="docutils literal"><span class="pre">prediction_error</span></span> method. The second one is just a prediction accuracy.</p>
<p>And finally, we run hyperparameter optimization.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="kn">import</span> <span class="nn">hyperopt</span>
<span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">partial</span>

<span class="c1"># Object stores all information about each trial.</span>
<span class="c1"># Also, it stores information about the best trial.</span>
<span class="n">trials</span> <span class="o">=</span> <span class="n">hyperopt</span><span class="o">.</span><span class="n">Trials</span><span class="p">()</span>

<span class="n">tpe</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span>
    <span class="n">hyperopt</span><span class="o">.</span><span class="n">tpe</span><span class="o">.</span><span class="n">suggest</span><span class="p">,</span>

    <span class="c1"># Sample 1000 candidate and select candidate that</span>
    <span class="c1"># has highest Expected Improvement (EI)</span>
    <span class="n">n_EI_candidates</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>

    <span class="c1"># Use 20% of best observations to estimate next</span>
    <span class="c1"># set of parameters</span>
    <span class="n">gamma</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>

    <span class="c1"># First 20 trials are going to be random</span>
    <span class="n">n_startup_jobs</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">hyperopt</span><span class="o">.</span><span class="n">fmin</span><span class="p">(</span>
    <span class="n">train_network</span><span class="p">,</span>

    <span class="n">trials</span><span class="o">=</span><span class="n">trials</span><span class="p">,</span>
    <span class="n">space</span><span class="o">=</span><span class="n">parameter_space</span><span class="p">,</span>

    <span class="c1"># Set up TPE for hyperparameter optimization</span>
    <span class="n">algo</span><span class="o">=</span><span class="n">tpe</span><span class="p">,</span>

    <span class="c1"># Maximum number of iterations. Basically it trains at</span>
    <span class="c1"># most 200 networks before selecting the best one.</span>
    <span class="n">max_evals</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<p>And after all trials, we can check the best one in the <span class="docutils literal"><span class="pre">trials.best_trial</span></span> attribute.</p>
</div>
<div class="section" id="disadvantages-of-tpe">
<h3><a class="toc-backref" href="#id27">Disadvantages of TPE</a></h3>
<p>On of the biggest disadvantages of this algorithm is that it selects parameters independently from each other. For instance, there is a clear relation between regularization and number of training epoch parameters. With regularization, we usually can train network for more epochs and with more epochs we can achieve better results. On the other hand without regularization, many epochs can be a bad choice because network starts overfitting and validation error increases. Without taking into account the state of the regularization variable each next choice for the number of epochs can look arbitrary.</p>
<p>It’s good in case if you now that some variables have relations. To overcome problem from the previous example you can construct two different choices for epochs. The first one will enable regularization and selects a number of epochs from the <span class="math">\([500, 1000]\)</span> range. And the second one without regularization and selects number of epochs from the <span class="math">\([10, 200]\)</span> range.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">'training_parameters'</span><span class="p">,</span> <span class="p">[</span>
    <span class="p">{</span>
        <span class="s1">'regularization'</span><span class="p">:</span> <span class="bp">True</span><span class="p">,</span>
        <span class="s1">'n_epochs'</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">quniform</span><span class="p">(</span><span class="s1">'n_epochs'</span><span class="p">,</span> <span class="mi">500</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="n">q</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
    <span class="p">},</span> <span class="p">{</span>
        <span class="s1">'regularization'</span><span class="p">:</span> <span class="bp">False</span><span class="p">,</span>
        <span class="s1">'n_epochs'</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">quniform</span><span class="p">(</span><span class="s1">'n_epochs'</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">300</span><span class="p">,</span> <span class="n">q</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
    <span class="p">},</span>
<span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="summary">
<h2><a class="toc-backref" href="#id28">Summary</a></h2>
<p>The Bayesian Optimization and TPE algorithms show great improvement over the classic hyperparameter optimization methods. They allow to learn from the training history and give better and better estimations for the next set of parameters. But it still takes lots of time to apply these algorithms. It’s great if you have an access to multiple machines and you can parallel parameter tuning procedure <a class="footnote-reference" href="#id8" id="id4">[4]</a>, but usually, it’s not an option. Sometimes it’s better just to avoid hyperparameter optimization. In case if you just try to build a network for trivial problems like image classification it’s better to use existed architectures with pre-trained parameters like <a class="reference external" href="https://github.com/itdxer/neupy/tree/master/examples/cnn/alexnet.py">AlexNet</a>, <a class="reference external" href="https://github.com/itdxer/neupy/tree/master/examples/cnn/vgg19.py">VGG19</a> or <a class="reference external" href="https://github.com/itdxer/neupy/tree/master/examples/cnn/resnet50.py">ResNet</a>.</p>
<p>For unique problems that don’t have pre-trained networks the classic and simple hand-tuning is a great way to start. A few iterations can give you a good architecture which won’t be the state-of-the-art but should give you satisfying result with a minimum of problems. In case if accuracy does not suffice your needs you can always boost your performance getting more data or developing ensembles with different models.</p>
</div>
<div class="section" id="source-code">
<h2><a class="toc-backref" href="#id29">Source Code</a></h2>
<p>All source code is available on GitHub in the <a class="reference external" href="https://github.com/itdxer/neupy/blob/master/notebooks/Hyperparameter%20optimization%20for%20Neural%20Networks.ipynb">iPython notebook</a>. It includes all visualizations and hyperparameter selection algorithms.</p>
</div>
<div class="section" id="references">
<h2><a class="toc-backref" href="#id30">References</a></h2>
<table class="docutils footnote" frame="void" id="id5" rules="none">
<colgroup><col class="label"/><col/></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id3">[1]</a></td><td>Bayesian Optimization and Acquisition Functions from <a class="reference external" href="http://www.cse.wustl.edu/~garnett/cse515t/files/lecture_notes/12.pdf">http://www.cse.wustl.edu/~garnett/cse515t/files/lecture_notes/12.pdf</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id6" rules="none">
<colgroup><col class="label"/><col/></colgroup>
<tbody valign="top">
<tr><td class="label">[2]</td><td>Gaussian Processes in Machine Learning from <a class="reference external" href="http://mlg.eng.cam.ac.uk/pub/pdf/Ras04.pdf">http://mlg.eng.cam.ac.uk/pub/pdf/Ras04.pdf</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id7" rules="none">
<colgroup><col class="label"/><col/></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id2">[3]</a></td><td>Slides: Introduction to Gaussian Process from <a class="reference external" href="https://www.cs.toronto.edu/~hinton/csc2515/notes/gp_slides_fall08.pdf">https://www.cs.toronto.edu/~hinton/csc2515/notes/gp_slides_fall08.pdf</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id8" rules="none">
<colgroup><col class="label"/><col/></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id4">[4]</a></td><td>Preliminary Evaluation of Hyperopt Algorithms on HPOLib from <a class="reference external" href="http://compneuro.uwaterloo.ca/files/publications/bergstra.2014.pdf">http://compneuro.uwaterloo.ca/files/publications/bergstra.2014.pdf</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id9" rules="none">
<colgroup><col class="label"/><col/></colgroup>
<tbody valign="top">
<tr><td class="label">[5]</td><td>Algorithms for Hyper-Parameter Optimization from <a class="reference external" href="http://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization.pdf">http://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization.pdf</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id10" rules="none">
<colgroup><col class="label"/><col/></colgroup>
<tbody valign="top">
<tr><td class="label">[6]</td><td>Slides: Pattern Recognition, Lecture 6 from <a class="reference external" href="http://www.csd.uwo.ca/~olga/Courses/CS434a_541a/Lecture6.pdf">http://www.csd.uwo.ca/~olga/Courses/CS434a_541a/Lecture6.pdf</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id11" rules="none">
<colgroup><col class="label"/><col/></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id1">[7]</a></td><td>Low-discrepancy sampling methods from <a class="reference external" href="http://planning.cs.uiuc.edu/node210.html">http://planning.cs.uiuc.edu/node210.html</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id12" rules="none">
<colgroup><col class="label"/><col/></colgroup>
<tbody valign="top">
<tr><td class="label">[8]</td><td>Parzen-Window Density Estimation from <a class="reference external" href="https://www.cs.utah.edu/~suyash/Dissertation_html/node11.html">https://www.cs.utah.edu/~suyash/Dissertation_html/node11.html</a></td></tr>
</tbody>
</table>
</div>
]]></description>
             <pubDate>Sat, 17 Dec 2016 00:00:00 +0100</pubDate>
        </item>
    
        <item>
            <link>http://neupy.com/2016/11/12/mnist_classification.html</link>
            <guid>http://neupy.com/2016/11/12/mnist_classification.html</guid>
            <title><![CDATA[MNIST Classification]]></title>
            <description><![CDATA[<span id="id1"/><h1>MNIST Classification</h1>
<p>The MNIST problem is probably the most known for those who have already
heared about neural networks. This short tutorial shows you how to build prediction models in NeuPy. Let’s start developing model.</p>
<p>First of all we need to load data.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span><span class="p">,</span> <span class="n">model_selection</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mnist</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">fetch_mldata</span><span class="p">(</span><span class="s1">'MNIST original'</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="n">mnist</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">mnist</span><span class="o">.</span><span class="n">target</span>
</pre></div>
</div>
<p>I used scikit-learn to fetch the MNIST dataset, but you can load it in different way.</p>
<p>Data doesn’t have appropriate format for neural network, so we need to make simple transformation before use it.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">OneHotEncoder</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="n">data</span> <span class="o">/</span> <span class="mf">255.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="n">data</span> <span class="o">-</span> <span class="n">data</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target_scaler</span> <span class="o">=</span> <span class="n">OneHotEncoder</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target</span> <span class="o">=</span> <span class="n">target_scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">target</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">todense</span><span class="p">()</span>
</pre></div>
</div>
<p>Next we need to divide dataset into two parts: train and test. Regarding <a class="reference external" href="http://yann.lecun.com/exdb/mnist/">The
MNIST Database</a> page we will use 60,000
samples for training and 10,000 for test.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">neupy</span> <span class="kn">import</span> <span class="n">environment</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">environment</span><span class="o">.</span><span class="n">reproducible</span><span class="p">()</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x_train</span><span class="p">,</span> <span class="n">x_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">data</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span>
<span class="gp">... </span>    <span class="n">target</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span>
<span class="gp">... </span>    <span class="n">train_size</span><span class="o">=</span><span class="p">(</span><span class="mf">6.</span> <span class="o">/</span> <span class="mi">7</span><span class="p">)</span>
<span class="gp">... </span><span class="p">)</span>
</pre></div>
</div>
<p>In the previous procedure I converted all data to <cite>float32</cite> data type. This
simple trick will help us use less memory and decrease computational time.
Theano is a main backend for the Gradient Descent based algorithms in NeuPy.
For the Theano we need to add additional configuration that will explain Theano that
we are going to use 32bit float numbers.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">theano</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">theano</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">floatX</span> <span class="o">=</span> <span class="s1">'float32'</span>
</pre></div>
</div>
<p>We prepared everything that we need for the neural network training. Now we are
able to create the neural network that will classify digits for us.</p>
<p>Let’s start with an architecture. I didn’t reinvent the wheel and used one of the know architectures from <a class="reference external" href="http://yann.lecun.com/exdb/mnist/">The Database</a> page which is 784 &gt; 500 &gt; 300 &gt; 10. As the main activation function I used Relu and Softmax for the final layer. The main algorithm is a Nesterov Momentum that uses 100 samples per batch iteration. Actually all this and other network configuration should be clear from the code shown below.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">neupy</span> <span class="kn">import</span> <span class="n">algorithms</span><span class="p">,</span> <span class="n">layers</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">network</span> <span class="o">=</span> <span class="n">algorithms</span><span class="o">.</span><span class="n">Momentum</span><span class="p">(</span>
<span class="gp">... </span>    <span class="p">[</span>
<span class="gp">... </span>        <span class="n">layers</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="mi">784</span><span class="p">),</span>
<span class="gp">... </span>        <span class="n">layers</span><span class="o">.</span><span class="n">Relu</span><span class="p">(</span><span class="mi">500</span><span class="p">),</span>
<span class="gp">... </span>        <span class="n">layers</span><span class="o">.</span><span class="n">Relu</span><span class="p">(</span><span class="mi">300</span><span class="p">),</span>
<span class="gp">... </span>        <span class="n">layers</span><span class="o">.</span><span class="n">Softmax</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span>
<span class="gp">... </span>    <span class="p">],</span>
<span class="gp">... </span>    <span class="n">error</span><span class="o">=</span><span class="s1">'categorical_crossentropy'</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">step</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">verbose</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">shuffle_data</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">momentum</span><span class="o">=</span><span class="mf">0.99</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">nesterov</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
<span class="gp">... </span><span class="p">)</span>
</pre></div>
</div>
<p>Isn’t it simple and clear? All the most important information related to the neural network you can find in the terminal output. If you run the code that shown above you would get the same output as on the figure below.</p>
<a class="reference internal image-reference" href="http://neupy.com/_images/bpnet-config-logs.png"><img alt="Gradient Descent configuration" class="align-center" src="http://neupy.com/_images/bpnet-config-logs.png" style="width: 70%;"/></a>
<p>From this output we can extract a lot of information about network configurations.</p>
<p>First of all, as we can see, most of options have green color label, but some of them are gray. Green color defines all options which we put in network manually and gray color options are default parameters. All properties separeted on few groups and each group is a <a class="reference internal" href="http://neupy.com/apidocs/neupy.algorithms.gd.momentum.html#neupy.algorithms.gd.momentum.Momentum" title="neupy.algorithms.gd.momentum.Momentum"><span class="xref py py-class docutils literal"><span class="pre">Momentum</span></span></a>  parent classes. More information about <a class="reference internal" href="http://neupy.com/apidocs/neupy.algorithms.gd.momentum.html#neupy.algorithms.gd.momentum.Momentum" title="neupy.algorithms.gd.momentum.Momentum"><span class="xref py py-class docutils literal"><span class="pre">Momentum</span></span></a> algorithm properties you will find in documentation, just click on algorithm name link and you will see it.</p>
<p>In addition for feedforward neural networks it’s possible to check architecture in form of a table.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="gp">&gt;&gt;&gt; </span><span class="n">network</span><span class="o">.</span><span class="n">architecture</span><span class="p">()</span>
</pre></div>
</div>
<a class="reference internal image-reference" href="http://neupy.com/_images/bpnet-architecture.png"><img alt="Neural Network Architecture" class="align-center" src="http://neupy.com/_images/bpnet-architecture.png" style="width: 70%;"/></a>
<p>Now we are going to train network. Let set up 20 epochs for training procedure and check the result.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="gp">&gt;&gt;&gt; </span><span class="n">network</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
</pre></div>
</div>
<p>Output in terminal should look similar to this one:</p>
<a class="reference internal image-reference" href="http://neupy.com/_images/bpnet-train-logs.png"><img alt="GradientDescent training procedure output" class="align-center" src="http://neupy.com/_images/bpnet-train-logs.png" style="width: 70%;"/></a>
<p>Output show the most important information related to training procedure. Each epoch contains 4 columns. First one identified epoch. The second one show training error. The third one is optional. In case you have validation dataset, you can check learning perfomanse using dataset separated from the learning procedure. And the last column shows how many time network trains during this epoch.</p>
<p>From the table is not clear network’s training progress. We can check it very easy. Network instance contains built-in method that build line plot that show training progress. Let’s check our progress.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">neupy</span> <span class="kn">import</span> <span class="n">plots</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">plots</span><span class="o">.</span><span class="n">error_plot</span><span class="p">(</span><span class="n">network</span><span class="p">)</span>
</pre></div>
</div>
<a class="reference internal image-reference" href="http://neupy.com/_images/bpnet-train-errors-plot.png"><img alt="GradientDescent epoch errors plot" class="align-center" src="http://neupy.com/_images/bpnet-train-errors-plot.png" style="width: 70%;"/></a>
<p>From the figure above you can notice that validation error does not decrease over time. Sometimes it goes up and sometimes down, but it doesn’t mean that network trains poorly. Let’s check small example that can make this problem clear.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="gp">&gt;&gt;&gt; </span><span class="n">actual_values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model1_prediction</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model2_prediction</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">])</span>
</pre></div>
</div>
<p>In the code above you can see two prediction releate to the different models. The first model predicted two samples right and one wrong. The second one predicted everything right. But second model’s predictions are less certain. Let’s check the cross entropy error.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">neupy</span> <span class="kn">import</span> <span class="n">estimators</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">estimators</span><span class="o">.</span><span class="n">binary_crossentropy</span><span class="p">(</span><span class="n">actual_values</span><span class="p">,</span> <span class="n">model1_prediction</span><span class="p">)</span>
<span class="go">0.3756706118583679</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">estimators</span><span class="o">.</span><span class="n">binary_crossentropy</span><span class="p">(</span><span class="n">actual_values</span><span class="p">,</span> <span class="n">model2_prediction</span><span class="p">)</span>
<span class="go">0.5108255743980408</span>
</pre></div>
</div>
<p>That is the result that we looked for. The second model made better prediction, but it got a higher cross entropy error. It means that we less certain about our prediction. Similar situation we’ve observed in the plot above.</p>
<p>Let’s finally make a simple report for our classification result.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">metrics</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_predicted</span> <span class="o">=</span> <span class="n">network</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_test</span><span class="p">)</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">y_test</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y_test</span><span class="p">))</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">print</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_predicted</span><span class="p">))</span>
<span class="go">        precision    recall  f1-score   support</span>

<span class="go">    0       0.98      0.99      0.99       936</span>
<span class="go">    1       0.99      0.99      0.99      1163</span>
<span class="go">    2       0.98      0.98      0.98       982</span>
<span class="go">    3       0.98      0.99      0.98      1038</span>
<span class="go">    4       0.98      0.98      0.98       948</span>
<span class="go">    5       0.99      0.98      0.98       921</span>
<span class="go">    6       0.99      0.99      0.99      1013</span>
<span class="go">    7       0.98      0.98      0.98      1029</span>
<span class="go">    8       0.98      0.98      0.98       978</span>
<span class="go">    9       0.98      0.96      0.97       992</span>

<span class="go">    avg / total       0.98      0.98      0.98     10000</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">score</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_predicted</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">print</span><span class="p">(</span><span class="s2">"Validation accuracy: {:.2%}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">score</span><span class="p">))</span>
<span class="go">Validation accuracy: 98.37%</span>
</pre></div>
</div>
<p>The 98.37% accuracy is pretty good for such a quick solution. Additional modification can improve prediction accuracy.</p>
]]></description>
             <pubDate>Sat, 12 Nov 2016 00:00:00 +0100</pubDate>
        </item>
    
        <item>
            <link>http://neupy.com/2015/09/21/password_recovery.html</link>
            <guid>http://neupy.com/2015/09/21/password_recovery.html</guid>
            <title><![CDATA[Password recovery]]></title>
            <description><![CDATA[<span id="id1"/><h1><a class="toc-backref" href="#id2">Password recovery</a></h1>
<div class="contents topic" id="contents">
<p class="topic-title first">Contents</p>
<ul class="simple">
<li><a class="reference internal" href="http://neupy.com/2015/09/21/password_recovery.html#password-recovery" id="id2">Password recovery</a><ul>
<li><a class="reference internal" href="http://neupy.com/2015/09/21/password_recovery.html#data-transformation" id="id3">Data transformation</a></li>
<li><a class="reference internal" href="http://neupy.com/2015/09/21/password_recovery.html#saving-password-into-the-network" id="id4">Saving password into the network</a></li>
<li><a class="reference internal" href="http://neupy.com/2015/09/21/password_recovery.html#recovering-password-from-the-network" id="id5">Recovering password from the network</a></li>
<li><a class="reference internal" href="http://neupy.com/2015/09/21/password_recovery.html#test-it-using-monte-carlo" id="id6">Test it using Monte Carlo</a></li>
<li><a class="reference internal" href="http://neupy.com/2015/09/21/password_recovery.html#possible-problems" id="id7">Possible problems</a></li>
<li><a class="reference internal" href="http://neupy.com/2015/09/21/password_recovery.html#summary" id="id8">Summary</a></li>
<li><a class="reference internal" href="http://neupy.com/2015/09/21/password_recovery.html#download-script" id="id9">Download script</a></li>
</ul>
</li>
</ul>
</div>
<p>In this article we are going to build a simple neural network that will recover password from a broken one.
If you aren’t familiar with a <a class="reference internal" href="http://neupy.com/apidocs/neupy.algorithms.memory.discrete_hopfield_network.html#neupy.algorithms.memory.discrete_hopfield_network.DiscreteHopfieldNetwork" title="neupy.algorithms.memory.discrete_hopfield_network.DiscreteHopfieldNetwork"><span class="xref py py-class docutils literal"><span class="pre">Discrete</span> <span class="pre">Hopfield</span> <span class="pre">Network</span></span></a> algorithm, you can read <a class="reference internal" href="http://neupy.com/2015/09/20/discrete_hopfield_network.html#discrete-hopfield-network"><span>this article</span></a>.</p>
<p>Before running all experiments, we need to set up <span class="docutils literal"><span class="pre">seed</span></span> parameter to make all results reproducible.
But you can test code without it.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="kn">from</span> <span class="nn">neupy</span> <span class="kn">import</span> <span class="n">environment</span>
<span class="n">environment</span><span class="o">.</span><span class="n">reproducible</span><span class="p">()</span>
</pre></div>
</div>
<p>If you can’t reproduce with your version of Python or libraries you can install those ones that were used in this article:</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">neupy</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">neupy</span><span class="o">.</span><span class="n">__version__</span>
<span class="go">'0.3.0'</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">numpy</span><span class="o">.</span><span class="n">__version__</span>
<span class="go">'1.9.2'</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">platform</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">platform</span><span class="o">.</span><span class="n">python_version</span><span class="p">()</span>
<span class="go">'3.4.3'</span>
</pre></div>
</div>
<p>Code works with a Python 2.7 as well.</p>
<div class="section" id="data-transformation">
<h2><a class="toc-backref" href="#id3">Data transformation</a></h2>
<p>Before building the network that will save and recover passwords, we should make transformations for input and output data.
But it wouldn’t be enough just to encode it, we should set up a constant length for an input string to make sure that strings will have the same length
Also we should define what string encoding we will use.
For simplicity we will use only ASCII symbols.
So, let’s define a function that transforms a string into a binary list.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="k">def</span> <span class="nf">str2bin</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">30</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">text</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">max_length</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">"Text can't contains more "</span>
                         <span class="s2">"than {} symbols"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">max_length</span><span class="p">))</span>

    <span class="n">text</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">rjust</span><span class="p">(</span><span class="n">max_length</span><span class="p">)</span>

    <span class="n">bits_list</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">symbol</span> <span class="ow">in</span> <span class="n">text</span><span class="p">:</span>
        <span class="n">bits</span> <span class="o">=</span> <span class="nb">bin</span><span class="p">(</span><span class="nb">ord</span><span class="p">(</span><span class="n">symbol</span><span class="p">))</span>
        <span class="c1"># Cut `0b` from the beggining and fill with zeros if they</span>
        <span class="c1"># are missed</span>
        <span class="n">bits</span> <span class="o">=</span> <span class="n">bits</span><span class="p">[</span><span class="mi">2</span><span class="p">:]</span><span class="o">.</span><span class="n">zfill</span><span class="p">(</span><span class="mi">8</span><span class="p">)</span>
        <span class="n">bits_list</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="n">bits</span><span class="p">))</span>

    <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="n">bits_list</span><span class="p">)</span>
</pre></div>
</div>
<p>Our function takes 2 parameters.
First one is the string that we want to encode.
And second attribute is setting up a constant length for input vector.
If length of the input string is less than <span class="docutils literal"><span class="pre">max_length</span></span> value, then function fills spaces at the beginning of the string.</p>
<p>Let’s check <span class="docutils literal"><span class="pre">str2bin</span></span> function output.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="gp">&gt;&gt;&gt; </span><span class="n">str2bin</span><span class="p">(</span><span class="s2">"test"</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="go">[0, 0, 1, 0, 0, 0, 0, 0, 0, ... ]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">len</span><span class="p">(</span><span class="n">str2bin</span><span class="p">(</span><span class="s2">"test"</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">5</span><span class="p">))</span>
<span class="go">40</span>
</pre></div>
</div>
<p>ASCII encoding uses 8 bits per symbol and we set up 5 symbols per string, so our vector length equals to 40.
From the first output, as you can see, first 8 symbols are equal to <span class="docutils literal"><span class="pre">00100000</span></span>, that is a space value from the ASCII table.</p>
<p>After preforming recovery procedure we will always be getting a binary list.
So before we begin to store data in neural network, we should define another function that transforms a binary list back into a string (which is basically inversed operation to the previous function).</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="k">def</span> <span class="nf">chunker</span><span class="p">(</span><span class="n">sequence</span><span class="p">,</span> <span class="n">size</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">position</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">sequence</span><span class="p">),</span> <span class="n">size</span><span class="p">):</span>
        <span class="k">yield</span> <span class="n">sequence</span><span class="p">[</span><span class="n">position</span><span class="p">:</span><span class="n">position</span> <span class="o">+</span> <span class="n">size</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">bin2str</span><span class="p">(</span><span class="n">array</span><span class="p">):</span>
    <span class="n">characters</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">binary_symbol_code</span> <span class="ow">in</span> <span class="n">chunker</span><span class="p">(</span><span class="n">array</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">8</span><span class="p">):</span>
        <span class="n">binary_symbol_str</span> <span class="o">=</span> <span class="s1">''</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="nb">str</span><span class="p">,</span> <span class="n">binary_symbol_code</span><span class="p">))</span>
        <span class="n">character</span> <span class="o">=</span> <span class="nb">chr</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">binary_symbol_str</span><span class="p">,</span> <span class="n">base</span><span class="o">=</span><span class="mi">2</span><span class="p">))</span>
        <span class="n">characters</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">character</span><span class="p">)</span>
    <span class="k">return</span> <span class="s1">''</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">characters</span><span class="p">)</span><span class="o">.</span><span class="n">lstrip</span><span class="p">()</span>
</pre></div>
</div>
<p>If we test this function we will get word <span class="docutils literal"><span class="pre">test</span></span> back.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="gp">&gt;&gt;&gt; </span><span class="n">bin2str</span><span class="p">(</span><span class="n">str2bin</span><span class="p">(</span><span class="s2">"test"</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">5</span><span class="p">))</span>
<span class="go">'test'</span>
</pre></div>
</div>
<p>Pay attention! Function has removed all spaces at the beggining of the string before bringing them back.
We assume that password won’t contain space at the beggining.</p>
</div>
<div class="section" id="saving-password-into-the-network">
<h2><a class="toc-backref" href="#id4">Saving password into the network</a></h2>
<p>Now we are ready to save the password into the network.
For this task we are going to define another function that create network and save password inside of it.
Let’s define this function and later we will look at it step by step.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">neupy</span> <span class="kn">import</span> <span class="n">algorithms</span>

<span class="k">def</span> <span class="nf">save_password</span><span class="p">(</span><span class="n">real_password</span><span class="p">,</span> <span class="n">noise_level</span><span class="o">=</span><span class="mi">5</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">noise_level</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">"`noise_level` must be equal or greater than 1."</span><span class="p">)</span>

    <span class="n">binary_password</span> <span class="o">=</span> <span class="n">str2bin</span><span class="p">(</span><span class="n">real_password</span><span class="p">)</span>
    <span class="n">bin_password_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">binary_password</span><span class="p">)</span>

    <span class="n">data</span> <span class="o">=</span> <span class="p">[</span><span class="n">binary_password</span><span class="p">]</span>

    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">noise_level</span><span class="p">):</span>
        <span class="c1"># The farther from the 0.5 value the less likely</span>
        <span class="c1"># password recovery</span>
        <span class="n">noise</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">binomial</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.55</span><span class="p">,</span> <span class="n">bin_password_len</span><span class="p">)</span>
        <span class="n">data</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">noise</span><span class="p">)</span>

    <span class="n">dhnet</span> <span class="o">=</span> <span class="n">algorithms</span><span class="o">.</span><span class="n">DiscreteHopfieldNetwork</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="s1">'sync'</span><span class="p">)</span>
    <span class="n">dhnet</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">data</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">dhnet</span>
</pre></div>
</div>
<p>If you have already read <a class="reference internal" href="http://neupy.com/2015/09/20/discrete_hopfield_network.html#discrete-hopfield-network"><span>Discrete Hopfield Network article</span></a>, you should know that if we add only one vector into the network we will get it dublicated or with reversed signs through the whole matrix.
To make it a little bit secure we can add some noise into the network.
For this reason we introduce one additional parameter <span class="docutils literal"><span class="pre">noise_level</span></span> into the function.
This parameter controls number of randomly generated binary vectors.
With each iteration using Binomial distribution we generate random binary vector with 55% probability of getting 1 in <cite>noise</cite> vector.
And then we put all the noise vectors and transformed password into one matrix.
And finaly we save all data in the <a class="reference internal" href="http://neupy.com/apidocs/neupy.algorithms.memory.discrete_hopfield_network.html#neupy.algorithms.memory.discrete_hopfield_network.DiscreteHopfieldNetwork" title="neupy.algorithms.memory.discrete_hopfield_network.DiscreteHopfieldNetwork"><span class="xref py py-class docutils literal"><span class="pre">Discrete</span> <span class="pre">Hopfield</span> <span class="pre">Network</span></span></a>.</p>
<p>And that’s it.
Function returns trained network for a later usage.</p>
<p>But why do we use random binary vectors instead of the decoded random strings?
The problem is in the similarity between two vectors.
Let’s check two approaches and compare them with a <a class="reference external" href="https://en.wikipedia.org/wiki/Hamming_distance">Hamming distance</a>.
But before starting we should define a function that measures distance between two vectors.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="kn">import</span> <span class="nn">string</span>
<span class="kn">import</span> <span class="nn">random</span>

<span class="k">def</span> <span class="nf">hamming_distance</span><span class="p">(</span><span class="n">left</span><span class="p">,</span> <span class="n">right</span><span class="p">):</span>
    <span class="n">left</span><span class="p">,</span> <span class="n">right</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">left</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">right</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">left</span><span class="o">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="n">right</span><span class="o">.</span><span class="n">shape</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">"Shapes must be equal"</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">left</span> <span class="o">!=</span> <span class="n">right</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">generate_password</span><span class="p">(</span><span class="n">min_length</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">30</span><span class="p">):</span>
    <span class="n">symbols</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span>
        <span class="n">string</span><span class="o">.</span><span class="n">ascii_letters</span> <span class="o">+</span>
        <span class="n">string</span><span class="o">.</span><span class="n">digits</span> <span class="o">+</span>
        <span class="n">string</span><span class="o">.</span><span class="n">punctuation</span>
    <span class="p">)</span>
    <span class="n">password_len</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">randrange</span><span class="p">(</span><span class="n">min_length</span><span class="p">,</span> <span class="n">max_length</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">password</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">symbols</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">password_len</span><span class="p">)]</span>
    <span class="k">return</span> <span class="s1">''</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">password</span><span class="p">)</span>
</pre></div>
</div>
<p>In addition you can see the <span class="docutils literal"><span class="pre">generate_password</span></span> function that we will use for tests.
Let’s check Hamming distance between two randomly generate password vectors.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="gp">&gt;&gt;&gt; </span><span class="n">hamming_distance</span><span class="p">(</span><span class="n">str2bin</span><span class="p">(</span><span class="n">generate_password</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">20</span><span class="p">)),</span>
<span class="gp">... </span>                 <span class="n">str2bin</span><span class="p">(</span><span class="n">generate_password</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">20</span><span class="p">)))</span>
<span class="go">70</span>
</pre></div>
</div>
<p>As we can see two randomly generated passwords are very similar to each other (approximetly 70% (<span class="math">\(100 * (240 - 70) / 240\)</span>) of bits are the same).
But If we compare randomly generated password to random binary vector we will see the difference.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="gp">&gt;&gt;&gt; </span><span class="n">hamming_distance</span><span class="p">(</span><span class="n">str2bin</span><span class="p">(</span><span class="n">generate_password</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">20</span><span class="p">)),</span>
<span class="gp">... </span>                 <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">binomial</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.55</span><span class="p">,</span> <span class="mi">240</span><span class="p">))</span>
<span class="go">134</span>
</pre></div>
</div>
<p>Hamming distance is bigger than in the previous example.
A little bit more than 55% of the bits are different.</p>
<p>The greater the difference between them the easier recovery procedure for the input vectors patterns from the network.
For this reason we use randomly generated binary vector instead of random password.</p>
<p>Of course it’s better to save not randomly generated noise vectors but randomly generated passwords converted into binary vectors, cuz if you use wrong input pattern randomly generated password might be recovered instead of the correct one.</p>
</div>
<div class="section" id="recovering-password-from-the-network">
<h2><a class="toc-backref" href="#id5">Recovering password from the network</a></h2>
<p>Now we are going to define the last function which will recover a password from the network.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="k">def</span> <span class="nf">recover_password</span><span class="p">(</span><span class="n">dhnet</span><span class="p">,</span> <span class="n">broken_password</span><span class="p">):</span>
    <span class="n">test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">str2bin</span><span class="p">(</span><span class="n">broken_password</span><span class="p">))</span>
    <span class="n">recovered_password</span> <span class="o">=</span> <span class="n">dhnet</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">test</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">recovered_password</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
        <span class="n">recovered_password</span> <span class="o">=</span> <span class="n">recovered_password</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:]</span>

    <span class="k">return</span> <span class="n">bin2str</span><span class="p">(</span><span class="n">recovered_password</span><span class="p">)</span>
</pre></div>
</div>
<p>Function takes two parameters.
The first one is network example from which function will recover a password from a broken one.
And the second parameter is a broken password.</p>
<p>Finnaly we can test password recovery from the network.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="gp">&gt;&gt;&gt; </span><span class="n">my_password</span> <span class="o">=</span> <span class="s2">"$My%Super^Secret*^&amp;Passwd"</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dhnet</span> <span class="o">=</span> <span class="n">save_password</span><span class="p">(</span><span class="n">my_password</span><span class="p">,</span> <span class="n">noise_level</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">recover_password</span><span class="p">(</span><span class="n">dhnet</span><span class="p">,</span> <span class="s2">"-My-Super-Secret---Passwd"</span><span class="p">)</span>
<span class="go">'$My%Super^Secret*^&amp;Passwd'</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">_</span> <span class="o">==</span> <span class="n">my_password</span>
<span class="go">True</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">recover_password</span><span class="p">(</span><span class="n">dhnet</span><span class="p">,</span> <span class="s2">"-My-Super"</span><span class="p">)</span>
<span class="go">'\x19`\xa0\x04Í\x14#ÛE2er\x1eÛe#2m4jV\x07PqsCwd'</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">recover_password</span><span class="p">(</span><span class="n">dhnet</span><span class="p">,</span> <span class="s2">"Invalid"</span><span class="p">)</span>
<span class="go">'\x02 \x1d`\x80$Ì\x1c#ÎE¢eò\x0eÛe§:/$ê\x04\x07@5sCu$'</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">recover_password</span><span class="p">(</span><span class="n">dhnet</span><span class="p">,</span> <span class="s2">"MySuperSecretPasswd"</span><span class="p">)</span>
<span class="go">'$My%Super^Secret*^&amp;Passwd'</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">_</span> <span class="o">==</span> <span class="n">my_password</span>
<span class="go">True</span>
</pre></div>
</div>
<p>Everithing looks fine.
After multiple times code running you can rarely find a problem.
Network can produce a string which wasn’t taught.
This string can look almost like a password with a few different symbols.
The problem appears when network creates additional local minimum somewhere between input patterns.
We can’t prevent it from running into the local minimum.
For more information about this problem you can check <a class="reference internal" href="http://neupy.com/2015/09/20/discrete_hopfield_network.html#discrete-hopfield-network"><span>article about Discrete Hopfield Network</span></a>.</p>
</div>
<div class="section" id="test-it-using-monte-carlo">
<h2><a class="toc-backref" href="#id6">Test it using Monte Carlo</a></h2>
<p>Let’s test our solution with randomly generated passwords.
For this task we can use Monte Carlo experiment.
At each step we create random password and try to recover it from a broken password.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="kn">import</span> <span class="nn">pprint</span>
<span class="kn">from</span> <span class="nn">operator</span> <span class="kn">import</span> <span class="n">itemgetter</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">OrderedDict</span>

<span class="k">def</span> <span class="nf">cutword</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">fromleft</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">fromleft</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">word</span><span class="p">[</span><span class="o">-</span><span class="n">k</span><span class="p">:]</span> <span class="k">if</span> <span class="n">k</span> <span class="o">!=</span> <span class="mi">0</span> <span class="k">else</span> <span class="s1">''</span><span class="p">)</span><span class="o">.</span><span class="n">rjust</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">word</span><span class="p">))</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">word</span><span class="p">[:</span><span class="n">k</span><span class="p">]</span> <span class="k">if</span> <span class="n">k</span> <span class="o">!=</span> <span class="mi">0</span> <span class="k">else</span> <span class="s1">''</span><span class="p">)</span><span class="o">.</span><span class="n">ljust</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">word</span><span class="p">))</span>

<span class="n">n_times</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">cases</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">([</span>
    <span class="p">(</span><span class="s1">'exclude-one'</span><span class="p">,</span> <span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)),</span>
    <span class="p">(</span><span class="s1">'exclude-quarter'</span><span class="p">,</span> <span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">x</span> <span class="o">//</span> <span class="mi">4</span><span class="p">)),</span>
    <span class="p">(</span><span class="s1">'exclude-half'</span><span class="p">,</span> <span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span> <span class="o">//</span> <span class="mi">2</span><span class="p">)),</span>
    <span class="p">(</span><span class="s1">'just-one-symbol'</span><span class="p">,</span> <span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="mi">1</span><span class="p">)),</span>
    <span class="p">(</span><span class="s1">'empty-string'</span><span class="p">,</span> <span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="mi">0</span><span class="p">)),</span>
<span class="p">])</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="o">.</span><span class="n">fromkeys</span><span class="p">(</span><span class="n">cases</span><span class="o">.</span><span class="n">keys</span><span class="p">(),</span> <span class="mi">0</span><span class="p">)</span>

<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_times</span><span class="p">):</span>
    <span class="n">real_password</span> <span class="o">=</span> <span class="n">generate_password</span><span class="p">(</span><span class="n">min_length</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">25</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">casename</span><span class="p">,</span> <span class="n">func</span> <span class="ow">in</span> <span class="n">cases</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">n_letters</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">real_password</span><span class="p">))</span>
        <span class="n">broken_password</span> <span class="o">=</span> <span class="n">cutword</span><span class="p">(</span><span class="n">real_password</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="n">n_letters</span><span class="p">,</span>
                                  <span class="n">fromleft</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

        <span class="n">dhnet</span> <span class="o">=</span> <span class="n">save_password</span><span class="p">(</span><span class="n">real_password</span><span class="p">,</span> <span class="n">noise_level</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
        <span class="n">recovered_password</span> <span class="o">=</span> <span class="n">recover_password</span><span class="p">(</span><span class="n">dhnet</span><span class="p">,</span> <span class="n">broken_password</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">recovered_password</span> <span class="o">!=</span> <span class="n">real_password</span><span class="p">:</span>
            <span class="n">results</span><span class="p">[</span><span class="n">casename</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>

<span class="k">print</span><span class="p">(</span><span class="s2">"Number of fails for each test case:"</span><span class="p">)</span>
<span class="n">pprint</span><span class="o">.</span><span class="n">pprint</span><span class="p">(</span><span class="n">results</span><span class="p">)</span>
</pre></div>
</div>
<p>After sumbmission your output should look the same as the one below (if you followed everything step by step):</p>
<div class="highlight-python"><div class="highlight"><pre><span/>Number of fails for each test case:
{'exclude-one': 11,
 'exclude-quarter': 729,
 'exclude-half': 5823,
 'just-one-symbol': 9998,
 'empty-string': 10000}
</pre></div>
</div>
<p>At this test we catch two situations when the network recovers the password from one symbol, which is not very good.
It really depends on the noise which we stored inside the network.
Randomization can’t give you perfect results.
Sometimes it can recover a password from an empty string, but such situation is also very rare.</p>
<p>In the last test, on each iteration we cut password from the left side and filled other parts with spaces.
Let’s test another approach.
Let’s cut a password from the right side and see what we’ll get:</p>
<div class="highlight-python"><div class="highlight"><pre><span/>Number of fails for each test case:
{'exclude-one': 17,
 'exclude-quarter': 705,
 'exclude-half': 5815,
 'just-one-symbol': 9995,
 'empty-string': 10000}
</pre></div>
</div>
<p>Results look similar to the previous test.</p>
<p>Another interesting test can take place if you randomly replace some symbols with spaces:</p>
<div class="highlight-python"><div class="highlight"><pre><span/>Number of fails for each test case:
{'exclude-one': 14,
 'exclude-quarter': 749,
 'exclude-half': 5760,
 'just-one-symbol': 9998,
 'empty-string': 10000}
</pre></div>
</div>
<p>The result is very similar to the previous two.</p>
<p>And finally, instead of replacing symbols with spaces we can remove symbols without any replacements.
Results do not look good:</p>
<div class="highlight-python"><div class="highlight"><pre><span/>Number of fails for each test case:
{'exclude-one': 3897,
 'exclude-quarter': 9464,
 'exclude-half': 9943,
 'just-one-symbol': 9998,
 'empty-string': 9998}
</pre></div>
</div>
<p>I guess in first case (<span class="docutils literal"><span class="pre">exclude-one</span></span>) we just got lucky and after eliminating one symbol from the end didn’t shift most of the symbols.
So removing symbols is not a very good idea.</p>
<p>All functions that you need for experiments you can find at the <a class="reference external" href="https://github.com/itdxer/neupy/tree/master/examples/memory/password_recovery.py">github</a>.</p>
</div>
<div class="section" id="possible-problems">
<h2><a class="toc-backref" href="#id7">Possible problems</a></h2>
<p>There are a few possible problems in the Discrete Hopfile Network.</p>
<ol class="arabic simple">
<li>As we saw from the last experiments, shifted passwords are harder to recover than the passwords with missed symbols. It’s better to replace missed symbols with some other things.</li>
<li>There already exists small probability for recovering passwords from empty strings.</li>
<li>Similar binary code representation for different symbols is a big problem. Sometimes you can have a situation where two symbols in binary code represantation are different just by one bit. The first solution is to use a One Hot Encoder. But it can give us even more problems. For example, we used symbols from list of 94 symbols for the password. If we encode each symbol we will get a vector with 93 zeros and just one active value. The problem is that after the recovery procedure we should always get 1 active value, but this situation is very unlikely to happen.</li>
</ol>
</div>
<div class="section" id="summary">
<h2><a class="toc-backref" href="#id8">Summary</a></h2>
<p>Despite some problems, network recovers passwords very well.
Monte Carlo experiment shows that the fewer symbols we know the less is probability for recovering them correctly.</p>
<p>Even this simple network can be a powerful tool if you know its limitations.</p>
</div>
<div class="section" id="download-script">
<h2><a class="toc-backref" href="#id9">Download script</a></h2>
<p>You can download and test a full script from the <a class="reference external" href="https://github.com/itdxer/neupy/tree/master/examples/memory/password_recovery.py">github repository</a>.</p>
<p>It doesn’t contain a fixed <span class="docutils literal"><span class="pre">environment.reproducible</span></span> function, so you will get different outputs after each run.</p>
</div>
]]></description>
             <pubDate>Mon, 21 Sep 2015 00:00:00 +0200</pubDate>
        </item>
    
        <item>
            <link>http://neupy.com/2015/09/20/discrete_hopfield_network.html</link>
            <guid>http://neupy.com/2015/09/20/discrete_hopfield_network.html</guid>
            <title><![CDATA[Discrete Hopfiel Network]]></title>
            <description><![CDATA[<span id="discrete-hopfield-network"/><h1><a class="toc-backref" href="#id5">Discrete Hopfiel Network</a></h1>
<div class="contents topic" id="contents">
<p class="topic-title first">Contents</p>
<ul class="simple">
<li><a class="reference internal" href="http://neupy.com/2015/09/20/discrete_hopfield_network.html#discrete-hopfiel-network" id="id5">Discrete Hopfiel Network</a><ul>
<li><a class="reference internal" href="http://neupy.com/2015/09/20/discrete_hopfield_network.html#architecture" id="id6">Architecture</a><ul>
<li><a class="reference internal" href="http://neupy.com/2015/09/20/discrete_hopfield_network.html#training-procedure" id="id7">Training procedure</a></li>
<li><a class="reference internal" href="http://neupy.com/2015/09/20/discrete_hopfield_network.html#recovery-from-memory" id="id8">Recovery from memory</a><ul>
<li><a class="reference internal" href="http://neupy.com/2015/09/20/discrete_hopfield_network.html#synchronous" id="id9">Synchronous</a></li>
<li><a class="reference internal" href="http://neupy.com/2015/09/20/discrete_hopfield_network.html#asynchronous" id="id10">Asynchronous</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="http://neupy.com/2015/09/20/discrete_hopfield_network.html#memory-limit" id="id11">Memory limit</a></li>
<li><a class="reference internal" href="http://neupy.com/2015/09/20/discrete_hopfield_network.html#hallucinations" id="id12">Hallucinations</a></li>
<li><a class="reference internal" href="http://neupy.com/2015/09/20/discrete_hopfield_network.html#example" id="id13">Example</a></li>
<li><a class="reference internal" href="http://neupy.com/2015/09/20/discrete_hopfield_network.html#more-reading" id="id14">More reading</a></li>
<li><a class="reference internal" href="http://neupy.com/2015/09/20/discrete_hopfield_network.html#references" id="id15">References</a></li>
</ul>
</li>
</ul>
</div>
<p>In this article we are going to learn about <a class="reference internal" href="http://neupy.com/apidocs/neupy.algorithms.memory.discrete_hopfield_network.html#neupy.algorithms.memory.discrete_hopfield_network.DiscreteHopfieldNetwork" title="neupy.algorithms.memory.discrete_hopfield_network.DiscreteHopfieldNetwork"><span class="xref py py-class docutils literal"><span class="pre">Discrete</span> <span class="pre">Hopfield</span> <span class="pre">Network</span></span></a> algorithm.</p>
<p><a class="reference internal" href="http://neupy.com/apidocs/neupy.algorithms.memory.discrete_hopfield_network.html#neupy.algorithms.memory.discrete_hopfield_network.DiscreteHopfieldNetwork" title="neupy.algorithms.memory.discrete_hopfield_network.DiscreteHopfieldNetwork"><span class="xref py py-class docutils literal"><span class="pre">Discrete</span> <span class="pre">Hopfield</span> <span class="pre">Network</span></span></a> is a type of algorithms which is called - <a class="reference external" href="https://en.wikipedia.org/wiki/Autoassociative_memory">Autoassociative memories</a>
Don’t be scared of the word <cite>Autoassociative</cite>.
The idea behind this type of algorithms is very simple.
It can store useful information in <cite>memory</cite> and later it is able to reproduce this information from partialy broken patterns.
You can preceive it as human memory.
For instance, imagine that you look at an old picture of a place where you were long time ago, but this picture is of very bad quality and very blurry.
By looking at the picture you manage to recognize a few objects or places that make sense to you and form some objects even though they are blurry.
It can be a house, a lake or anything that can add up to the whole picture and bring out some associations about this place.
With these details that you got from your memory so far other parts of picture start to make even more sense.
Though you don’t clearly see all objects in the picture, you start to remember things and withdraw from your memory some images, that cannot be seen in the picture, just because of those very familiarly-shaped details that you’ve got so far.
That’s what it is all about.
Autoassociative memory networks is a posibily to interprete functions of memory into neural network model.</p>
<p>Don’t worry if you have only basic knowledge in Linear Algebra; in this article I’ll try to explain the idea as simple as possible.
If you are interested in proofs of the <a class="reference internal" href="http://neupy.com/apidocs/neupy.algorithms.memory.discrete_hopfield_network.html#neupy.algorithms.memory.discrete_hopfield_network.DiscreteHopfieldNetwork" title="neupy.algorithms.memory.discrete_hopfield_network.DiscreteHopfieldNetwork"><span class="xref py py-class docutils literal"><span class="pre">Discrete</span> <span class="pre">Hopfield</span> <span class="pre">Network</span></span></a> you can check them at R. Rojas. Neural Networks <a class="footnote-reference" href="#id2" id="id1">[1]</a> book.</p>
<div class="section" id="architecture">
<h2><a class="toc-backref" href="#id6">Architecture</a></h2>
<p><a class="reference internal" href="http://neupy.com/apidocs/neupy.algorithms.memory.discrete_hopfield_network.html#neupy.algorithms.memory.discrete_hopfield_network.DiscreteHopfieldNetwork" title="neupy.algorithms.memory.discrete_hopfield_network.DiscreteHopfieldNetwork"><span class="xref py py-class docutils literal"><span class="pre">Discrete</span> <span class="pre">Hopfield</span> <span class="pre">Network</span></span></a> is an easy algorithm.
It’s simple because you don’t need a lot of background knowledge in Maths for using it.
Everything you need to know is how to make a basic Linear Algebra operations, like outer product or sum of two matrices.</p>
<p>Let’s begin with a basic thing.
What do we know about this neural network so far?
Just the name and the type.
From the name we can identify one useful thing about the network.
It’s <cite>Discrete</cite>.
It means that network only works with binary vectors.
But for this network we wouldn’t use binary numbers in a typical form.
Instead, we will use bipolar numbers.
They are almost the same, but instead of 0 we are going to use -1 to decode a negative state.
We can’t use zeros.
And there are two main reasons for it.
The first one is that zeros reduce information from the network weight, later in this article you are going to see it.
The second one is more complex, it depends on the nature of bipolar vectors.
Basically they are more likely to be orthogonal to each other which is a critical moment for the <a class="reference internal" href="http://neupy.com/apidocs/neupy.algorithms.memory.discrete_hopfield_network.html#neupy.algorithms.memory.discrete_hopfield_network.DiscreteHopfieldNetwork" title="neupy.algorithms.memory.discrete_hopfield_network.DiscreteHopfieldNetwork"><span class="xref py py-class docutils literal"><span class="pre">Discrete</span> <span class="pre">Hopfield</span> <span class="pre">Network</span></span></a>.
But as I mentioned before we won’t talk about proofs or anything not related to basic understanding of Linear Algebra operations.</p>
<p>So, let’s look at how we can train and use the <a class="reference internal" href="http://neupy.com/apidocs/neupy.algorithms.memory.discrete_hopfield_network.html#neupy.algorithms.memory.discrete_hopfield_network.DiscreteHopfieldNetwork" title="neupy.algorithms.memory.discrete_hopfield_network.DiscreteHopfieldNetwork"><span class="xref py py-class docutils literal"><span class="pre">Discrete</span> <span class="pre">Hopfield</span> <span class="pre">Network</span></span></a>.</p>
<div class="section" id="training-procedure">
<h3><a class="toc-backref" href="#id7">Training procedure</a></h3>
<p>We can’t use memory without any patterns stored in it.
So first of all we are going to learn how to train the network.
For the <a class="reference internal" href="http://neupy.com/apidocs/neupy.algorithms.memory.discrete_hopfield_network.html#neupy.algorithms.memory.discrete_hopfield_network.DiscreteHopfieldNetwork" title="neupy.algorithms.memory.discrete_hopfield_network.DiscreteHopfieldNetwork"><span class="xref py py-class docutils literal"><span class="pre">Discrete</span> <span class="pre">Hopfield</span> <span class="pre">Network</span></span></a> train procedure doesn’t require any iterations.
It includes just an outer product between input vector and transposed input vector.</p>
<div class="math">
\[\begin{split}\begin{align*}
    W = x \cdot x^T =
    \left[
    \begin{array}{c}
      x_1\\
      x_2\\
      \vdots\\
      x_n
    \end{array}
    \right]
    \cdot
    \left[
    \begin{array}{c}
      x_1 &amp; x_2 &amp; \cdots &amp; x_n
    \end{array}
    \right]
\end{align*}
=\end{split}\]</div>
<div class="math">
\[\begin{split}\begin{align*}
    =
    \left[
    \begin{array}{c}
      x_1^2 &amp; x_1 x_2 &amp; \cdots &amp; x_1 x_n \\
      x_2 x_1 &amp; x_2^2 &amp; \cdots &amp; x_2 x_n \\
      \vdots\\
      x_n x_1 &amp; x_n x_2 &amp; \cdots &amp; x_n^2 \\
    \end{array}
    \right]
\end{align*}\end{split}\]</div>
<p><span class="math">\(W\)</span> is a weight matrix and <span class="math">\(x\)</span> is an input vector.
Each value <span class="math">\(x_i\)</span> in the input vector can only be -1 or 1.
So on the matrix diagonal we only have squared values and it means we will always see 1s at those places.
Think about it, everytime, in any case, values on the diagonal can take just one possible state.
We can’t use this information, because it doesn’t say anything useful about patterns that are stored in the memory and even can make incorrect contribution into the output result.
For this reason we need to set up all the diagonal values equal to zero.
The final weight formula should look like this one below.</p>
<div class="math">
\[\begin{split}\begin{align*}
    W =
    x x^T - I =
    \left[
    \begin{array}{c}
      0 &amp; x_1 x_2 &amp; \cdots &amp; x_1 x_n \\
      x_2 x_1 &amp; 0 &amp; \cdots &amp; x_2 x_n \\
      \vdots\\
      x_n x_1 &amp; x_n x_2 &amp; \cdots &amp; 0 \\
    \end{array}
    \right]
\end{align*}\end{split}\]</div>
<p>Where <span class="math">\(I\)</span> is an identity matrix.</p>
<p>But usualy we need to store more values in memory.
For another pattern we have to do exacly the same procedure as before and then just add the generated weight matrix to the old one.</p>
<div class="math">
\[W = W_{old} + W_{new}\]</div>
<p>And this procedure generates us a new weight that would be valid for both previously stored patterns.
Later you can add other patterns using the same algorithm.</p>
<p>But if you need to store multiple vectors inside the network at the same time you don’t need to compute the weight for each vector and then sum them up.
If you have a matrix <span class="math">\(X \in \Bbb R^{m\times n}\)</span> where each row is the input vector, then you can just make product matrix between transposed input matrix and input matrix.</p>
<div class="math">
\[W = X^T X - m I\]</div>
<p>Where <span class="math">\(I\)</span> is an identity matrix (<span class="math">\(I \in \Bbb R^{n\times n}\)</span>), <span class="math">\(n\)</span> is a number of features in the input vector and <span class="math">\(m\)</span> is a number of input patterns inside the matrix <span class="math">\(X\)</span>.
Term <span class="math">\(m I\)</span> removes all values from the diagonal.
Basically we remove 1s for each stored pattern and since we have <span class="math">\(m\)</span> of them, we should do it <span class="math">\(m\)</span> times.
Practically, it’s not very good to create an identity matrix just to set up zeros on the diagonal, especially when dimension on the matrix is very big.
Usually linear algebra libraries give you a possibility to set up diagonal values without creating an additional matrix and this solution would be more efficient.
For example in NumPy library it’s a <a class="reference external" href="http://docs.scipy.org/doc/numpy/reference/generated/numpy.fill_diagonal.html">numpy.fill_diagonal</a> function.</p>
<p>Let’s check an example just to make sure that everything is clear.
Let’s pretend we have a vector <span class="math">\(u\)</span>.</p>
<div class="math">
\[\begin{split}u = \left[\begin{align*}1 \\ -1 \\ 1 \\ -1\end{align*}\right]\end{split}\]</div>
<p>Assume that network doesn’t have patterns inside of it, so the vector <span class="math">\(u\)</span> would be the first one.
Let’s compute weights for the network.</p>
<div class="math">
\[\begin{split}\begin{align*}
    U = u u^T =
    \left[
        \begin{array}{c}
            1 \\
            -1 \\
            1 \\
            -1
        \end{array}
    \right]
    \left[
        \begin{array}{c}
            1 &amp; -1 &amp; 1 &amp; -1
        \end{array}
    \right]
    =
    \left[
        \begin{array}{cccc}
            1 &amp; -1 &amp; 1 &amp; -1\\
            -1 &amp; 1 &amp; -1 &amp; 1\\
            1 &amp; -1 &amp; 1 &amp; -1\\
            -1 &amp; 1 &amp; -1 &amp; 1
        \end{array}
    \right]
\end{align*}\end{split}\]</div>
<p>Look closer to the matrix <span class="math">\(U\)</span> that we got.
Outer product just repeats vector 4 times with the same or inversed values.
First and third columns (or rows, it doesn’t matter, because matrix is symmetrical) are exacly the same as the input vector.
The second and fourth are also the same, but with an opposite sign.
That’s because in the vector <span class="math">\(u\)</span> we have 1 on the first and third places and -1 on the other.</p>
<p>To make weight from the <span class="math">\(U\)</span> matrix, we need to remove ones from the diagonal.</p>
<div class="math">
\[\begin{split}W = U - I = \left[
    \begin{array}{cccc}
        1 &amp; -1 &amp; 1 &amp; -1\\
        -1 &amp; 1 &amp; -1 &amp; 1\\
        1 &amp; -1 &amp; 1 &amp; -1\\
        -1 &amp; 1 &amp; -1 &amp; 1
    \end{array}
\right] -
\left[
    \begin{array}{cccc}
        1 &amp; 0 &amp; 0 &amp; 0\\
        0 &amp; 1 &amp; 0 &amp; 0\\
        0 &amp; 0 &amp; 1 &amp; 0\\
        0 &amp; 0 &amp; 0 &amp; 1
    \end{array}
\right] =\end{split}\]</div>
<div class="math">
\[\begin{split}= \left[
    \begin{array}{cccc}
        0 &amp; -1 &amp; 1 &amp; -1\\
        -1 &amp; 0 &amp; -1 &amp; 1\\
        1 &amp; -1 &amp; 0 &amp; -1\\
        -1 &amp; 1 &amp; -1 &amp; 0
    \end{array}
\right]\end{split}\]</div>
<p><span class="math">\(I\)</span> is the identity matrix and <span class="math">\(I \in \Bbb R^{n \times n}\)</span>, where <span class="math">\(n\)</span> is a number of features in the input vector.</p>
<p>When we have one stored vector inside the weights we don’t really need to remove 1s from the diagonal.
The main problem would appear when we have more than one vector stored in the weights.
Each value on the diagonal would be equal to the number of stored vectors in it.</p>
</div>
<div class="section" id="recovery-from-memory">
<h3><a class="toc-backref" href="#id8">Recovery from memory</a></h3>
<p>The main advantage of Autoassociative network is that it is able to recover pattern from the memory using just a partial information about the pattern.
There are already two main approaches to this situation, synchronous and asynchronous.
We are going to master both of them.</p>
<div class="section" id="synchronous">
<h4><a class="toc-backref" href="#id9">Synchronous</a></h4>
<p>Synchronous approach is much more easier for understanding, so we are going to look at it firstly.
To recover your pattern from memory you just need to multiply the weight matrix by the input vector.</p>
<div class="math">
\[\begin{split}\begin{align*}
    s = {W}\cdot{x}=
    \left[
    \begin{array}{cccc}
      w_{11} &amp; w_{12} &amp; \ldots &amp; w_{1n}\\
      w_{21} &amp; w_{22} &amp; \ldots &amp; w_{2n}\\
      \vdots &amp; \vdots &amp; \ddots &amp; \vdots\\
      w_{n1} &amp; w_{n2} &amp; \ldots &amp; w_{nn}
    \end{array}
    \right]
    \left[
    \begin{array}{c}
      x_1\\
      x_2\\
      \vdots\\
      x_n
    \end{array}
    \right]
    =
\end{align*}\end{split}\]</div>
<div class="math">
\[\begin{split}\begin{align*}
    =
    \left[
        \begin{array}{c}
          w_{11}x_1+w_{12}x_2 + \cdots + w_{1n} x_n\\
          w_{21}x_1+w_{22}x_2 + \cdots + w_{2n} x_n\\
          \vdots\\
          w_{n1}x_1+w_{n2}x_2 + \cdots + w_{nn} x_n\\
        \end{array}
    \right]
\end{align*}\end{split}\]</div>
<p>Let’s analyze the result.
We summed up all information from the weights where each value can be any integer with an absolute value equal to or smaller than the number of patterns inside the network.
It’s clear that total sum value for <span class="math">\(s_i\)</span> is not necessary equal to -1 or 1, so we have to make additional operations that will make bipolar vector from the vector <span class="math">\(s\)</span>.</p>
<p>Let’s think about this product operation.
What does it actualy do?
Basically after training procedure we saved our pattern dublicated <span class="math">\(n\)</span> times (where <span class="math">\(n\)</span> is a number of input vector features) inside the weight.
When we store more patterns we get interception between them (it’s called a <strong>crosstalk</strong>) and each pattern add some noise to other patterns.
So, after perfoming product matrix between <span class="math">\(W\)</span> and <span class="math">\(x\)</span> for each value from the vector <span class="math">\(x\)</span> we’ll get a recovered vector with a little bit of noise.
For <span class="math">\(x_1\)</span> we get a first column from the matrix <span class="math">\(W\)</span>, for the <span class="math">\(x_2\)</span> a second column, and so on.
Then we sum up all vectors together.
This operation can remind you of voting.
For example we have 3 vectors.
If the first two vectors have 1 in the first position and the third one has -1 at the same position, the winner should be 1.
We can perform the same procedure with <span class="math">\(sign\)</span> function.
So the output value should be 1 if total value is greater then zero and -1 otherwise.</p>
<div class="math">
\[\begin{split}sign(x) = \left\{
    \begin{array}{lr}
        &amp;1 &amp;&amp; : x \ge 0\\
        &amp;-1 &amp;&amp; : x &lt; 0
    \end{array}
\right.\\\end{split}\]\[y = sign(s)\]</div>
<p>That’s it.
Now <span class="math">\(y\)</span> store the recovered pattern from the input vector <span class="math">\(x\)</span>.</p>
<p>Maybe now you can see why we can’t use zeros in the input vectors.
In <cite>voting</cite> procedure we use each row that was multiplied by bipolar number, but if values had been zeros they would have ignored columns from the weight matrix and we would have used only values related to ones in the input pattern.</p>
<p>Of course you can use 0 and 1 values and sometime you will get the correct result, but this approach give you much worse results than explained above.</p>
</div>
<div class="section" id="asynchronous">
<h4><a class="toc-backref" href="#id10">Asynchronous</a></h4>
<p>Previous approach is good, but it has some limitations.
If you change one value in the input vector it can change your output result and value won’t converge to the known pattern.
Another popular approach is an <strong>asynchronous</strong>.
This approach is more likely to remind you of real memory.
At the same time in network activates just one random neuron instead of all of them.
In terms of neural networks we say that <strong>neuron fires</strong>.
We iteratively repeat this operation multiple times and after some point network will converge to some pattern.</p>
<p>Let’s look at this example:
Consider that we already have a weight matrix <span class="math">\(W\)</span> with one pattern <span class="math">\(x\)</span>  inside of it.</p>
<div class="math">
\[\begin{split}\begin{align*}
    W =
    \left[
    \begin{array}{cccc}
      0 &amp; 1 &amp; -1 \\
      1 &amp; 0 &amp; -1 \\
      -1 &amp; -1 &amp; 0
    \end{array}
    \right]
\end{align*}\end{split}\]</div>
<div class="math">
\[\begin{split}\begin{align*}
    x =
    \left[
        \begin{array}{c}
          1\\
          1\\
          -1
        \end{array}
    \right]
\end{align*}\end{split}\]</div>
<p>Let’s assume that we have a vector <span class="math">\(x^{'}\)</span> from which we want to recover the pattern.</p>
<div class="math">
\[\begin{split}\begin{align*}
    x^{'} =
    \left[
        \begin{array}{c}
          1\\
          -1\\
          -1
        \end{array}
    \right]
\end{align*}\end{split}\]</div>
<p>In first iteration one neuron fires.
Let it be the second one.
So we multiply the first column by this selected value.</p>
<div class="math">
\[\begin{split}\begin{align*}
    x^{'}_2 =
    sign(\left[
        \begin{array}{c}
          1 &amp; -1 &amp; -1
        \end{array}
    \right] \cdot \left[
        \begin{array}{c}
          1\\
          0\\
          -1
        \end{array}
    \right]) = sign(2) = 1
\end{align*}\end{split}\]</div>
<p>And after this operation we set up a new value into the input vector <span class="math">\(x\)</span>.</p>
<div class="math">
\[\begin{split}\begin{align*}
    x^{'} =
    \left[
        \begin{array}{c}
          1\\
          1\\
          -1
        \end{array}
    \right]
\end{align*}\end{split}\]</div>
<p>As you can see after first iteration value is exacly the same as <span class="math">\(x\)</span> but we can keep going.
In second iteration random neuron fires again.
Let’s pretend that this time it was the third neuron.</p>
<div class="math">
\[\begin{split}\begin{align*}
    x^{'}_3 =
    sign(\left[
        \begin{array}{c}
          1 &amp; 1 &amp; -1
        \end{array}
    \right] \cdot \left[
        \begin{array}{c}
          -1\\
          -1\\
          0
        \end{array}
    \right]) = sign(-2) = -1
\end{align*}\end{split}\]</div>
<p><span class="math">\(x^{'}_3\)</span> is exacly the same as in the <span class="math">\(x^{'}\)</span> vector so we don’t need to update it.
We can repeat it as many times as we want, but we will be getting the same value.</p>
</div>
</div>
</div>
<div class="section" id="memory-limit">
<h2><a class="toc-backref" href="#id11">Memory limit</a></h2>
<p>Obviously, you can’t store infinite number of vectors inside the network.
There are two good rules of thumb.</p>
<p>Concider that <span class="math">\(n\)</span> is the dimension (number of features) of your input vector and <span class="math">\(m\)</span> is the number of patterns that you want to store in the network.
The first rule gives us a simple ration between <span class="math">\(m\)</span> and <span class="math">\(n\)</span>.</p>
<div class="math">
\[m \approx 0.18 n\]</div>
<p>The main problem with this rule is that proof assumes that stored vectors inside the weight are completly random with an equal probability.
Unfortunately, that is not always true.
Let’s suppose we save some images of numbers from 0 to 9.
Pictures are black and white, so we can encode them in bipolar vectors.
Will the probabilities be the same for seeing as many white pixels as black ones?
Usually no.
More likely that number of white pixels would be greater than number of black ones.
Before use this rule you have to think about type of your input patterns.</p>
<p>The second rule uses a logarithmic proportion.</p>
<div class="math">
\[m = \left \lfloor \frac{n}{2 \cdot log(n)} \right \rfloor\]</div>
<p>Both of these rules are good assumtions about the nature of data and its possible limits in memory.
Of course you can find situations when these rules will fail.</p>
</div>
<div class="section" id="hallucinations">
<h2><a class="toc-backref" href="#id12">Hallucinations</a></h2>
<p>Hallucinations is one of the main problems in the <a class="reference internal" href="http://neupy.com/apidocs/neupy.algorithms.memory.discrete_hopfield_network.html#neupy.algorithms.memory.discrete_hopfield_network.DiscreteHopfieldNetwork" title="neupy.algorithms.memory.discrete_hopfield_network.DiscreteHopfieldNetwork"><span class="xref py py-class docutils literal"><span class="pre">Discrete</span> <span class="pre">Hopfield</span> <span class="pre">Network</span></span></a>.
Sometimes network output can be something that we hasn’t taught it.</p>
<p>To understand this phenomena we should firstly define the Hopfield energy function.</p>
<div class="math">
\[E = -\frac{1}{2} \sum_{i=1}^{n} \sum_{j=1}^{n} w_{ij} x_i x_j + \sum_{i=1}^{n} \theta_i x_i\]</div>
<p>Where <span class="math">\(w_{ij}\)</span> is a weight value on the <span class="math">\(i\)</span>-th row and <span class="math">\(j\)</span>-th column.
<span class="math">\(x_i\)</span> is a <span class="math">\(i\)</span>-th values from the input vector <span class="math">\(x\)</span>.
<span class="math">\(\theta\)</span> is a threshold.
Threshold defines the bound to the sign function.
For this reason <span class="math">\(\theta\)</span> is equal to 0 for the <a class="reference internal" href="http://neupy.com/apidocs/neupy.algorithms.memory.discrete_hopfield_network.html#neupy.algorithms.memory.discrete_hopfield_network.DiscreteHopfieldNetwork" title="neupy.algorithms.memory.discrete_hopfield_network.DiscreteHopfieldNetwork"><span class="xref py py-class docutils literal"><span class="pre">Discrete</span> <span class="pre">Hopfield</span> <span class="pre">Network</span></span></a>.
In terms of a linear algebra we can write formula for the <a class="reference internal" href="http://neupy.com/apidocs/neupy.algorithms.memory.discrete_hopfield_network.html#neupy.algorithms.memory.discrete_hopfield_network.DiscreteHopfieldNetwork" title="neupy.algorithms.memory.discrete_hopfield_network.DiscreteHopfieldNetwork"><span class="xref py py-class docutils literal"><span class="pre">Discrete</span> <span class="pre">Hopfield</span> <span class="pre">Network</span></span></a> energy Function more simpler.</p>
<div class="math">
\[E = -\frac{1}{2} x^T W x\]</div>
<p>But linear algebra notation works only with the <span class="math">\(x\)</span> vector, we can’t use matrix <span class="math">\(X\)</span> with multiple input patterns instead of the <span class="math">\(x\)</span> in this formula.
For the energy function we’re always interested in finding a minimum value, for this reason it has minus sign at the beggining.</p>
<p>Let’s try to visualize it.
Assume that values for vector <span class="math">\(x\)</span> can be continous in order and we can visualize them using two parameters.
Let’s pretend that we have two vectors <cite>[1, -1]</cite> and <cite>[-1, 1]</cite> stored inside the network.
Below you can see the plot that visualizes energy function for this situation.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="http://neupy.com/_images/energy-function.png"><img alt="Energy function visualization for the network with two neurons" src="http://neupy.com/_images/energy-function.png" style="width: 80%;"/></a>
</div>
<p>As you can see we have two minimum values at the same points as those patterns that are already stored inside the network.
But between these two patterns function creates a saddle point somewhere at the point with coordinates <span class="math">\((0, 0)\)</span>.
In this case we can’t stick to the points <span class="math">\((0, 0)\)</span>.
But in situation with more dimensions this saddle points can be at the level of available values and they could be hallucination.
Unfortunately, we are very limited in terms of numbers of dimensions we could plot, but the problem is still the same.</p>
<p>Full source code for this plot you can find on <a class="reference external" href="https://github.com/itdxer/neupy/tree/master/examples/memory/dhn_energy_func.py">github</a></p>
</div>
<div class="section" id="example">
<h2><a class="toc-backref" href="#id13">Example</a></h2>
<p>Now we are ready for a more practical example.
Let’s define a few images that we are going to teach the network.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">neupy</span> <span class="kn">import</span> <span class="n">algorithms</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">draw_bin_image</span><span class="p">(</span><span class="n">image_matrix</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">image_matrix</span><span class="o">.</span><span class="n">tolist</span><span class="p">():</span>
<span class="gp">... </span>        <span class="k">print</span><span class="p">(</span><span class="s1">'| '</span> <span class="o">+</span> <span class="s1">' '</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s1">' *'</span><span class="p">[</span><span class="n">val</span><span class="p">]</span> <span class="k">for</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">row</span><span class="p">))</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">zero</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matrix</span><span class="p">([</span>
<span class="gp">... </span>    <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span>
<span class="gp">... </span><span class="p">])</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">one</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matrix</span><span class="p">([</span>
<span class="gp">... </span>    <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
<span class="gp">... </span><span class="p">])</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">two</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matrix</span><span class="p">([</span>
<span class="gp">... </span>    <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span>
<span class="gp">... </span><span class="p">])</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">draw_bin_image</span><span class="p">(</span><span class="n">zero</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">)))</span>
<span class="go">|   * * *</span>
<span class="go">| *       *</span>
<span class="go">| *       *</span>
<span class="go">| *       *</span>
<span class="go">| *       *</span>
<span class="go">|   * * *</span>
</pre></div>
</div>
<p>We have 3 images, so now we can train network with these patterns.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">zero</span><span class="p">,</span> <span class="n">one</span><span class="p">,</span> <span class="n">two</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dhnet</span> <span class="o">=</span> <span class="n">algorithms</span><span class="o">.</span><span class="n">DiscreteHopfieldNetwork</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="s1">'sync'</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dhnet</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</pre></div>
</div>
<p>That’s all. Now to make sure that network has memorized patterns right we can define the broken patterns and check how the network will recover them.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="gp">&gt;&gt;&gt; </span><span class="n">half_zero</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matrix</span><span class="p">([</span>
<span class="gp">... </span>    <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">draw_bin_image</span><span class="p">(</span><span class="n">half_zero</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">)))</span>
<span class="go">|   * * *</span>
<span class="go">| *       *</span>
<span class="go">| *       *</span>
<span class="go">|</span>
<span class="go">|</span>
<span class="go">|</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">half_two</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matrix</span><span class="p">([</span>
<span class="gp">... </span>    <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span>
<span class="gp">... </span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">draw_bin_image</span><span class="p">(</span><span class="n">half_two</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">)))</span>
<span class="go">|</span>
<span class="go">|</span>
<span class="go">|</span>
<span class="go">|   * *</span>
<span class="go">| *</span>
<span class="go">| * * * * *</span>
</pre></div>
</div>
<p>Now we can reconstruct pattern from the memory.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="gp">&gt;&gt;&gt; </span><span class="n">result</span> <span class="o">=</span> <span class="n">dhnet</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">half_zero</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">draw_bin_image</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">)))</span>
<span class="go">|   * * *</span>
<span class="go">| *       *</span>
<span class="go">| *       *</span>
<span class="go">| *       *</span>
<span class="go">| *       *</span>
<span class="go">|   * * *</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">result</span> <span class="o">=</span> <span class="n">dhnet</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">half_two</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">draw_bin_image</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">)))</span>
<span class="go">| * * *</span>
<span class="go">|       *</span>
<span class="go">|       *</span>
<span class="go">|   * *</span>
<span class="go">| *</span>
<span class="go">| * * * * *</span>
</pre></div>
</div>
<p>Cool! Network catches the pattern right.</p>
<p>But not always we will get the correct answer. Let’s define another broken pattern and check network output.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="gp">&gt;&gt;&gt; </span><span class="n">half_two</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matrix</span><span class="p">([</span>
<span class="gp">... </span>    <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span><span class="p">])</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">result</span> <span class="o">=</span> <span class="n">dhnet</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">half_two</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">draw_bin_image</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">)))</span>
<span class="go">|   * *</span>
<span class="go">|     *</span>
<span class="go">|     *</span>
<span class="go">|   * *</span>
<span class="go">| *   *</span>
<span class="go">| * * * * *</span>
</pre></div>
</div>
<p>We hasn’t clearly taught the network to deal with such pattern. But if we look closer, it looks like mixed pattern of numbers 1 and 2.</p>
<p>This problem we can solve using the asynchronous network approach. We don’t necessary need to create a new network, we can just simply switch its mode.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">neupy</span> <span class="kn">import</span> <span class="n">environment</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">environment</span><span class="o">.</span><span class="n">reproducible</span><span class="p">()</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dhnet</span><span class="o">.</span><span class="n">mode</span> <span class="o">=</span> <span class="s1">'async'</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dhnet</span><span class="o">.</span><span class="n">n_times</span> <span class="o">=</span> <span class="mi">400</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">result</span> <span class="o">=</span> <span class="n">dhnet</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">half_two</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">draw_bin_image</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">)))</span>
<span class="go">|   * *</span>
<span class="go">|     *</span>
<span class="go">|     *</span>
<span class="go">|     *</span>
<span class="go">|     *</span>
<span class="go">|     *</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">result</span> <span class="o">=</span> <span class="n">dhnet</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">half_two</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">draw_bin_image</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">)))</span>
<span class="go">| * * *</span>
<span class="go">|       *</span>
<span class="go">|       *</span>
<span class="go">|   * *</span>
<span class="go">| *</span>
<span class="go">| * * * *</span>
</pre></div>
</div>
<p>Our broken pattern is really close to the minimum of 1 and 2 patterns. Randomization helps us choose direction but it’s not nessesary the right one, especialy when the broken pattern is close to 1 and 2 at the same time.</p>
<p>Check last output with number two again. Is that a realy valid pattern for number 2? Final symbol in output is wrong. We are not able to recover patter 2 from this network, because input vector is always much closer to the minimum that looks very similar to pattern 2.</p>
<p>In plot below you can see first 200 iterations of the recovery procedure. Energy value was decreasing after each iteration until it reached the local minimum where pattern is equal to 2.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="http://neupy.com/_images/hopfield-energy-vis.png"><img alt="Asynchronous Discrete Hopfield Network energy update after each iteration" src="http://neupy.com/_images/hopfield-energy-vis.png" style="width: 80%;"/></a>
</div>
<p>And finally we can look closer to the network memory using Hinton diagram.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">neupy</span> <span class="kn">import</span> <span class="n">plots</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">12</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">"Hinton diagram"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">plots</span><span class="o">.</span><span class="n">hinton</span><span class="p">(</span><span class="n">dhnet</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<div class="figure align-center">
<a class="reference internal image-reference" href="http://neupy.com/_images/hinton-diagram.png"><img alt="Asynchronous Discrete Hopfield Network energy update after each iteration" src="http://neupy.com/_images/hinton-diagram.png" style="width: 80%;"/></a>
</div>
<p>This graph above shows the network weight matrix and all information stored inside of it. Hinton diagram is a very simple technique for the weight visualization in neural networks. Each value encoded in square where its size is an absolute value from the weight matrix and color shows the sign of this value. White is a positive and black is a negative. Usualy Hinton diagram helps identify some patterns in the weight matrix.</p>
<p>Let’s go back to the graph. What can you say about the network just by looking at this picture? First of all you can see that there is no squares on the diagonal. That is because they are equal to zero. The second important thing you can notice is that the plot is symmetrical. But that is not all that you can withdraw from the graph. Can you see different patterns? You can find rows or columns with exacly the same values, like the second and third columns. Fifth column is also the same but its sign is reversed. Now look closer to the antidiagonal. What can you say about it? If you are thinking that all squares are white - you are right. But why is that true? Is there always the same patterns for each memory matrix? No, it is a special property of patterns that we stored inside of it. If you draw a horizontal line in the middle of each image and look at it you will see that values are opposite symmetric. For instance, <span class="math">\(x_1\)</span> opposite symmetric to <span class="math">\(x_{30}\)</span>, <span class="math">\(x_2\)</span> to <span class="math">\(x_{29}\)</span>, <span class="math">\(x_3\)</span> to <span class="math">\(x_{28}\)</span> and so on. Zero pattern is a perfect example where each value have exacly the same opposite symmetric pair. One is almost perfect except one value on the <span class="math">\(x_2\)</span> position. Two is not clearly opposite symmetric. But if you check each value you will find that more than half of values are symmetrical. Combination of those patterns gives us a diagonal with all positive values. If we have all perfectly opposite symmetric patterns then squares on the antidiagonal will have the same length, but in this case pattern for number 2 gives a little bit of noise and squares have different sizes.</p>
<p>Properties that we’ve reviewed so far are just the most interesting and maybe other patterns you can encounter on your own.</p>
</div>
<div class="section" id="more-reading">
<h2><a class="toc-backref" href="#id14">More reading</a></h2>
<p>In addition you can read another article about a ‘<a class="reference internal" href="http://neupy.com/2015/09/21/password_recovery.html#password-recovery"><span>Password recovery</span></a>‘ from the memory using the <a class="reference internal" href="http://neupy.com/apidocs/neupy.algorithms.memory.discrete_hopfield_network.html#neupy.algorithms.memory.discrete_hopfield_network.DiscreteHopfieldNetwork" title="neupy.algorithms.memory.discrete_hopfield_network.DiscreteHopfieldNetwork"><span class="xref py py-class docutils literal"><span class="pre">Discrete</span> <span class="pre">Hopfield</span> <span class="pre">Network</span></span></a>.</p>
</div>
<div class="section" id="references">
<h2><a class="toc-backref" href="#id15">References</a></h2>
<table class="docutils footnote" frame="void" id="id2" rules="none">
<colgroup><col class="label"/><col/></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id1">[1]</a></td><td>R. Rojas. Neural Networks. In Associative Networks. pp. 311 - 336, 1996.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id3" rules="none">
<colgroup><col class="label"/><col/></colgroup>
<tbody valign="top">
<tr><td class="label">[2]</td><td>Math4IQB. (2013, November 17). Hopfield Networks. Retrieved
from <a class="reference external" href="https://www.youtube.com/watch?v=gfPUWwBkXZY">https://www.youtube.com/watch?v=gfPUWwBkXZY</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id4" rules="none">
<colgroup><col class="label"/><col/></colgroup>
<tbody valign="top">
<tr><td class="label">[3]</td><td>R. Callan. The Essence of Neural Networks. In Pattern Association. pp. 84 - 98, 1999.</td></tr>
</tbody>
</table>
</div>
]]></description>
             <pubDate>Sun, 20 Sep 2015 00:00:00 +0200</pubDate>
        </item>
    
        <item>
            <link>http://neupy.com/2015/07/04/boston_house_prices_dataset.html</link>
            <guid>http://neupy.com/2015/07/04/boston_house_prices_dataset.html</guid>
            <title><![CDATA[Predict prices for houses in the area of Boston]]></title>
            <description><![CDATA[<span id="boston-house-price"/><h1>Predict prices for houses in the area of Boston</h1>
<p>For this article we are going to predict house prices using Conjugate Gradient algorithm.</p>
<p>For the beginning we should load the data.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_boston</span><span class="p">()</span>
<span class="n">data</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">dataset</span><span class="o">.</span><span class="n">target</span>
</pre></div>
</div>
<p>Let’s look closer into the data.</p>
<table border="1" class="dataframe">
  <thead>
    <tr>
      <th>CRIM</th>
      <th>ZN</th>
      <th>INDUS</th>
      <th>CHAS</th>
      <th>NOX</th>
      <th>RM</th>
      <th>AGE</th>
      <th>DIS</th>
      <th>RAD</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0.00632</td>
      <td>18</td>
      <td>2.31</td>
      <td>0</td>
      <td>0.538</td>
      <td>6.575</td>
      <td>65.2</td>
      <td>4.0900</td>
      <td>1</td>
    </tr>
    <tr>
      <td>0.02731</td>
      <td>0</td>
      <td>7.07</td>
      <td>0</td>
      <td>0.469</td>
      <td>6.421</td>
      <td>78.9</td>
      <td>4.9671</td>
      <td>2</td>
    </tr>
    <tr>
      <td>0.02729</td>
      <td>0</td>
      <td>7.07</td>
      <td>0</td>
      <td>0.469</td>
      <td>7.185</td>
      <td>61.1</td>
      <td>4.9671</td>
      <td>2</td>
    </tr>
    <tr>
      <td>0.03237</td>
      <td>0</td>
      <td>2.18</td>
      <td>0</td>
      <td>0.458</td>
      <td>6.998</td>
      <td>45.8</td>
      <td>6.0622</td>
      <td>3</td>
    </tr>
    <tr>
      <td>0.06905</td>
      <td>0</td>
      <td>2.18</td>
      <td>0</td>
      <td>0.458</td>
      <td>7.147</td>
      <td>54.2</td>
      <td>6.0622</td>
      <td>3</td>
    </tr>
  </tbody>
</table>

<table border="1" class="dataframe">
  <thead>
    <tr>
      <th>TAX</th>
      <th>PTRATIO</th>
      <th>B</th>
      <th>LSTAT</th>
      <th>MEDV</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>296</td>
      <td>15.3</td>
      <td>396.90</td>
      <td>4.98</td>
      <td>24.0</td>
    </tr>
    <tr>
      <td>242</td>
      <td>17.8</td>
      <td>396.90</td>
      <td>9.14</td>
      <td>21.6</td>
    </tr>
    <tr>
      <td>242</td>
      <td>17.8</td>
      <td>392.83</td>
      <td>4.03</td>
      <td>34.7</td>
    </tr>
    <tr>
      <td>222</td>
      <td>18.7</td>
      <td>394.63</td>
      <td>2.94</td>
      <td>33.4</td>
    </tr>
    <tr>
      <td>222</td>
      <td>18.7</td>
      <td>396.90</td>
      <td>5.33</td>
      <td>36.2</td>
    </tr>
  </tbody>
</table><p>Data contains 14 columns.
The last column <span class="docutils literal"><span class="pre">MEDV</span></span> is a median value of owner-occupied homes in $1000’s.
The goal is to predict this prices.
Other columns we can use for Neural Network training.
All columns description you can find below.</p>
<ul class="simple">
<li>CRIM     per capita crime rate by town</li>
<li>ZN       proportion of residential land zoned for lots over 25,000 sq.ft.</li>
<li>INDUS    proportion of non-retail business acres per town</li>
<li>CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)</li>
<li>NOX      nitric oxides concentration (parts per 10 million)</li>
<li>RM       average number of rooms per dwelling</li>
<li>AGE      proportion of owner-occupied units built prior to 1940</li>
<li>DIS      weighted distances to five Boston employment centres</li>
<li>RAD      index of accessibility to radial highways</li>
<li>TAX      full-value property-tax rate per $10,000</li>
<li>PTRATIO  pupil-teacher ratio by town</li>
<li>B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town</li>
<li>LSTAT    % lower status of the population</li>
</ul>
<p>From data set description we can find that there are 13 continuous attributes (including “class” attribute “MEDV”) and 1 binary-valued attribute.
There is no multiple categorical data, so we don’t need to change feature dimension.
But we already have one problem.
If you look closer, you will find that every column has its own data range.
This situation is a bad thing for Neural Network training, because input values ​​make different contributions to the calculation of the output values.
Bigger values will be more important for Network which can be perceived as invalid assumption based on data.
For example in the first row, in the table above, column <span class="docutils literal"><span class="pre">B</span></span> contains value <cite>396.90</cite> and column <span class="docutils literal"><span class="pre">CRIM</span></span> - <cite>0.00632</cite>.
To fix this issue we should transfrom all columns to get similar ranges.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">preprocessing</span>

<span class="n">data_scaler</span> <span class="o">=</span> <span class="n">preprocessing</span><span class="o">.</span><span class="n">MinMaxScaler</span><span class="p">()</span>
<span class="n">target_scaler</span> <span class="o">=</span> <span class="n">preprocessing</span><span class="o">.</span><span class="n">MinMaxScaler</span><span class="p">()</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">data_scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="n">target</span> <span class="o">=</span> <span class="n">target_scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">target</span><span class="p">)</span>
</pre></div>
</div>
<p>After transformation data looks like this.</p>
<table border="1" class="dataframe">
  <thead>
    <tr>
      <th>CRIM</th>
      <th>ZN</th>
      <th>INDUS</th>
      <th>CHAS</th>
      <th>NOX</th>
      <th>...</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0.000000</td>
      <td>0.18</td>
      <td>0.067815</td>
      <td>0</td>
      <td>0.314815</td>
      <td>...</td>
    </tr>
    <tr>
      <td>0.000236</td>
      <td>0.00</td>
      <td>0.242302</td>
      <td>0</td>
      <td>0.172840</td>
      <td>...</td>
    </tr>
    <tr>
      <td>0.000236</td>
      <td>0.00</td>
      <td>0.242302</td>
      <td>0</td>
      <td>0.172840</td>
      <td>...</td>
    </tr>
    <tr>
      <td>0.000293</td>
      <td>0.00</td>
      <td>0.063050</td>
      <td>0</td>
      <td>0.150206</td>
      <td>...</td>
    </tr>
    <tr>
      <td>0.000705</td>
      <td>0.00</td>
      <td>0.063050</td>
      <td>0</td>
      <td>0.150206</td>
      <td>...</td>
    </tr>
  </tbody>
</table><p>All the data is now in the range between 0 and 1.</p>
<p>Then we should split our data set into train and validation.
We use 85% of data for train.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">neupy</span> <span class="kn">import</span> <span class="n">environment</span>

<span class="n">environment</span><span class="o">.</span><span class="n">reproducible</span><span class="p">()</span>

<span class="n">x_train</span><span class="p">,</span> <span class="n">x_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">data</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">train_size</span><span class="o">=</span><span class="mf">0.85</span>
<span class="p">)</span>
</pre></div>
</div>
<p>Now we are ready to build Neural Network which will predict house prices.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="kn">from</span> <span class="nn">neupy</span> <span class="kn">import</span> <span class="n">algorithms</span><span class="p">,</span> <span class="n">layers</span>

<span class="n">cgnet</span> <span class="o">=</span> <span class="n">algorithms</span><span class="o">.</span><span class="n">ConjugateGradient</span><span class="p">(</span>
    <span class="n">connection</span><span class="o">=</span><span class="p">[</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="mi">13</span><span class="p">),</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">(</span><span class="mi">50</span><span class="p">),</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
    <span class="p">],</span>
    <span class="n">search_method</span><span class="o">=</span><span class="s1">'golden'</span><span class="p">,</span>
    <span class="n">show_epoch</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span>
    <span class="n">verbose</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="n">addons</span><span class="o">=</span><span class="p">[</span><span class="n">algorithms</span><span class="o">.</span><span class="n">LinearSearch</span><span class="p">],</span>
<span class="p">)</span>
</pre></div>
</div>
<div class="figure align-center">
<a class="reference internal image-reference" href="http://neupy.com/_images/cgnet-init.png"><img alt="Conjgate Gradient train" src="http://neupy.com/_images/cgnet-init.png" style="width: 80%;"/></a>
</div>
<p>We define network with one hidden layer.
Input size for this layer is 50.
This value is just a guess.
For better and more accurate result we should choose it with other methods, but for now we can use this value.
As the main algorithm we take Conjugate Gradient.
This implementation of backpropagation is a little bit different from main interpretation of Conjugate Gradient.
For GradientDescent implementation we can’t guarantee that we get the local minimum in n-th steps (where <cite>n</cite> is the dimension).
To optimize it we should use linear search.
It will fix and set up better steps for Conjugate Gradient.</p>
<p>Now we are going to train the network.
For training we set up 100 epochs.
Also we will add test data into training function to check validation error on every epoch.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="n">cgnet</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
</pre></div>
</div>
<div class="figure align-center">
<a class="reference internal image-reference" href="http://neupy.com/_images/cgnet-train.png"><img alt="Conjgate Gradient train" src="http://neupy.com/_images/cgnet-train.png" style="width: 80%;"/></a>
</div>
<p>To make sure that all training processes go in a right way we can check erros updates while the training is in process.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="kn">from</span> <span class="nn">neupy</span> <span class="kn">import</span> <span class="n">plots</span>
<span class="n">plots</span><span class="o">.</span><span class="n">error_plot</span><span class="p">(</span><span class="n">cgnet</span><span class="p">)</span>
</pre></div>
</div>
<div class="figure align-center">
<a class="reference internal image-reference" href="http://neupy.com/_images/cgnet-error-plot.png"><img alt="Conjgate Gradient train" src="http://neupy.com/_images/cgnet-error-plot.png" style="width: 80%;"/></a>
</div>
<p>Error minimization procedure looks fine.
The problem is, that last error doesn’t show us the full picture of prediction accuracy.
Our output is always between zero and one and we count the results always into Mean Square Error.
To fix it, we are going to inverse our transformation for predicted and actual values and for accuracy measurment we will use Root Mean Square Logarithmic Error (RMSLE).</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="kn">from</span> <span class="nn">neupy.estimators</span> <span class="kn">import</span> <span class="n">rmsle</span>

<span class="n">y_predict</span> <span class="o">=</span> <span class="n">cgnet</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_test</span><span class="p">)</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">error</span> <span class="o">=</span> <span class="n">rmsle</span><span class="p">(</span><span class="n">target_scaler</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">y_test</span><span class="p">),</span>
              <span class="n">target_scaler</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">y_predict</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="n">error</span><span class="p">)</span>
</pre></div>
</div>
<p>Now we can see that our error approximately equals to <cite>0.22</cite> which is pretty small.
In the table below you can find 10 randomly chosen errors.</p>
<table border="1" class="dataframe">
  <thead>
    <tr>
      <th>Actual</th>
      <th>Predicted</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>31.2</td>
      <td>27.5</td>
    </tr>
    <tr>
      <td>18.7</td>
      <td>18.5</td>
    </tr>
    <tr>
      <td>20.1</td>
      <td>18.5</td>
    </tr>
    <tr>
      <td>17.2</td>
      <td>9.5</td>
    </tr>
    <tr>
      <td>8.3</td>
      <td>9.5</td>
    </tr>
    <tr>
      <td>50.0</td>
      <td>41.0</td>
    </tr>
    <tr>
      <td>42.8</td>
      <td>32.0</td>
    </tr>
    <tr>
      <td>20.5</td>
      <td>18.5</td>
    </tr>
    <tr>
      <td>16.8</td>
      <td>23.0</td>
    </tr>
    <tr>
      <td>11.8</td>
      <td>9.5</td>
    </tr>
  </tbody>
</table><p>The results are good for the first network implementation.
There are a lot of things which we can do to improve network results, but we will discuss them in an another article.</p>
]]></description>
             <pubDate>Sat, 04 Jul 2015 00:00:00 +0200</pubDate>
        </item>
    
        <item>
            <link>http://neupy.com/2015/07/04/visualize_backpropagation_algorithms.html</link>
            <guid>http://neupy.com/2015/07/04/visualize_backpropagation_algorithms.html</guid>
            <title><![CDATA[Visualize Algorithms based on the Backpropagation]]></title>
            <description><![CDATA[<h1><a class="toc-backref" href="#id3">Visualize Algorithms based on the Backpropagation</a></h1>
<div class="contents topic" id="contents">
<p class="topic-title first">Contents</p>
<ul class="simple">
<li><a class="reference internal" href="http://neupy.com/2015/07/04/visualize_backpropagation_algorithms.html#visualize-algorithms-based-on-the-backpropagation" id="id3">Visualize Algorithms based on the Backpropagation</a><ul>
<li><a class="reference internal" href="http://neupy.com/2015/07/04/visualize_backpropagation_algorithms.html#checking-data" id="id4">Checking data</a></li>
<li><a class="reference internal" href="http://neupy.com/2015/07/04/visualize_backpropagation_algorithms.html#initialize-contour" id="id5">Initialize contour</a></li>
<li><a class="reference internal" href="http://neupy.com/2015/07/04/visualize_backpropagation_algorithms.html#id1" id="id6">Visualize algorithms based on the Backpropagation</a><ul>
<li><a class="reference internal" href="http://neupy.com/2015/07/04/visualize_backpropagation_algorithms.html#gradient-descent" id="id7">Gradient Descent</a></li>
<li><a class="reference internal" href="http://neupy.com/2015/07/04/visualize_backpropagation_algorithms.html#momentum" id="id8">Momentum</a></li>
<li><a class="reference internal" href="http://neupy.com/2015/07/04/visualize_backpropagation_algorithms.html#rprop" id="id9">RPROP</a></li>
<li><a class="reference internal" href="http://neupy.com/2015/07/04/visualize_backpropagation_algorithms.html#irprop" id="id10">iRPROP+</a></li>
<li><a class="reference internal" href="http://neupy.com/2015/07/04/visualize_backpropagation_algorithms.html#gradient-descent-and-golden-search" id="id11">Gradient Descent and Golden Search</a></li>
</ul>
</li>
<li><a class="reference internal" href="http://neupy.com/2015/07/04/visualize_backpropagation_algorithms.html#bring-them-all-together" id="id12">Bring them all together</a></li>
<li><a class="reference internal" href="http://neupy.com/2015/07/04/visualize_backpropagation_algorithms.html#summary" id="id13">Summary</a></li>
</ul>
</li>
</ul>
</div>
<p>In this article we will be testing different algorithms based on the backpropagation method, visualizing them and trying to figure out some important features from a blog that we will get.</p>
<div class="section" id="checking-data">
<h2><a class="toc-backref" href="#id4">Checking data</a></h2>
<p>First of all we need to define simple dataset which contains 6 points with two features.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="n">input_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
    <span class="p">[</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">],</span>
<span class="p">])</span>
<span class="n">target_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
    <span class="p">[</span><span class="mi">1</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">1</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">1</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">0</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">0</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">0</span><span class="p">],</span>
<span class="p">])</span>
</pre></div>
</div>
<p>So we can make a scatter plot and look closer at this dots.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">input_data</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">input_data</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">target_data</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<div class="figure align-center">
<a class="reference internal image-reference" href="http://neupy.com/_images/bp-vis-scatter.png"><img alt="Dataset scatter plot" src="http://neupy.com/_images/bp-vis-scatter.png" style="width: 80%;"/></a>
</div>
<p>From the figure above we can clearly see that all dots are linearly separable and we are able to solve this problem with simple perceptron. But the goal of this article is to make clear visualization of learning process for different algorithm based on the backpropagation method, so the problem has to be as simple as possible, because in other cases it will be complex to visualize.</p>
<p>So, since the problem is linear separable we can solve it without hidden layers in network. There are two features and two classes, so we can build network which will take 2 input values and will produce 1 output. We need just two weights, so we can visualize them in contour plot.</p>
</div>
<div class="section" id="initialize-contour">
<h2><a class="toc-backref" href="#id5">Initialize contour</a></h2>
<p>I won’t  add all code related to the plots building in the article. In case if you are interested you can check the main script <a class="reference external" href="https://github.com/itdxer/neupy/blob/master/examples/mlp/gd_algorithms_visualization.py">here</a>.</p>
<a class="reference internal image-reference" href="http://neupy.com/_images/raw-contour-plot.png"><img alt="Approximation function contour plot" class="align-center" src="http://neupy.com/_images/raw-contour-plot.png" style="width: 80%;"/></a>
<p>The plot above shows error rate that depends on the network’s weights. The best result corresponds to the smallest error value. The best weights combination for this problem should be near the bottom right corner in the white area.</p>
<p>Next, we are going to look at 5 algorithms based on the Backpropagation. They are:</p>
<ul class="simple">
<li>Gradient descent</li>
<li>Momentum</li>
<li>RPROP</li>
<li>iRPROP+</li>
<li>Gradient Descent + Golden Search</li>
</ul>
<p>Let’s define start point for our algorithms. I’ve chosen the <cite>(-4, -4)</cite> point, because at this point network gives bad results and it will be interesting to observe the learning progress from a bad initialization point. In the script you can set up any other starting point you like.</p>
<p>This function will train the network until the error will be smaller than <cite>0.125</cite>. Every network starts at place with coordinates <cite>(-4, -4)</cite> and finishes near the point with the error value lower than <cite>0.125</cite>.</p>
</div>
<div class="section" id="id1">
<h2><a class="toc-backref" href="#id6">Visualize algorithms based on the Backpropagation</a></h2>
<div class="section" id="gradient-descent">
<h3><a class="toc-backref" href="#id7">Gradient Descent</a></h3>
<p>Let’s primarily check <a class="reference internal" href="http://neupy.com/apidocs/neupy.algorithms.gd.base.html#neupy.algorithms.gd.base.GradientDescent" title="neupy.algorithms.gd.base.GradientDescent"><span class="xref py py-class docutils literal"><span class="pre">Gradient</span> <span class="pre">Descent</span></span></a>.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="http://neupy.com/_images/bp-steps.png"><img alt="Weight update steps for the Gradient Descent" src="http://neupy.com/_images/bp-steps.png" style="width: 80%;"/></a>
</div>
<p>Gradient Descent got to the value close to 0.125 using 797 steps and this black curve is just tiny steps of gradient descent algorithm. We can zoom it and look even closer.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="http://neupy.com/_images/bp-steps-zoom.png"><img alt="Zoomed weight update steps for the Gradient Descent" src="http://neupy.com/_images/bp-steps-zoom.png" style="width: 80%;"/></a>
</div>
<p>Now we can see some information about gradient descent algorithm. All steps for gradient descent algorithm have approximately similar magnitude. Their direction doesn’t vary because contours in the zoomed picture are parallel to each other and in it we can see that there are still a lot of steps that are needed to be made to achieve the minimum. Also we can see that small vectors are perpendicular to the contour.</p>
<p>The problem is that the step size is a very sensitive parameter for the gradient descent. In typical problem we won’t be able to visualize the learning progress and we won’t have an ability to see that our updates over the epochs are inefficient. For this result I’ve used step size equal to <span class="docutils literal"><span class="pre">0.3</span></span>, but if we increased it to <span class="docutils literal"><span class="pre">10</span></span> we would reach our goal in <span class="docutils literal"><span class="pre">25</span></span> steps. I haven’t added any improvements to make a fair comparison to other algorithms in the summary chapter.</p>
</div>
<div class="section" id="momentum">
<h3><a class="toc-backref" href="#id8">Momentum</a></h3>
<p>Now let’s look at another very popular algorithm - <a class="reference internal" href="http://neupy.com/apidocs/neupy.algorithms.gd.momentum.html#neupy.algorithms.gd.momentum.Momentum" title="neupy.algorithms.gd.momentum.Momentum"><span class="xref py py-class docutils literal"><span class="pre">Momentum</span></span></a>.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="http://neupy.com/_images/momentum-steps.png"><img alt="Momentum steps" src="http://neupy.com/_images/momentum-steps.png" style="width: 80%;"/></a>
</div>
<p><a class="reference internal" href="http://neupy.com/apidocs/neupy.algorithms.gd.momentum.html#neupy.algorithms.gd.momentum.Momentum" title="neupy.algorithms.gd.momentum.Momentum"><span class="xref py py-class docutils literal"><span class="pre">Momentum</span></span></a> got to the value close to 0.125 by 92 steps, which is more than 8 times less than for the gradient descent. The basic idea behind <a class="reference internal" href="http://neupy.com/apidocs/neupy.algorithms.gd.momentum.html#neupy.algorithms.gd.momentum.Momentum" title="neupy.algorithms.gd.momentum.Momentum"><span class="xref py py-class docutils literal"><span class="pre">Momentum</span></span></a> algorithm is that it accumulates gradients from the previous epochs. It means that if the gradient has the same direction after each epoch weight update vector magnitude will increase. But if the gradient stars changing its direction weight update vector magnitude will decrease. Check the figure again. Imagine that you’re standing at a skatepark. Than you throw a ball into a half-pipe in a way that makes it roll smoothly on the surface. While it rolls down the gravity force drags it down and it makes the ball roll faster and faster. Let’s get back to the <a class="reference internal" href="http://neupy.com/apidocs/neupy.algorithms.gd.momentum.html#neupy.algorithms.gd.momentum.Momentum" title="neupy.algorithms.gd.momentum.Momentum"><span class="xref py py-class docutils literal"><span class="pre">Momentum</span></span></a> algorithm and try to find these properties in the plot.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="http://neupy.com/_images/momentum-steps-zoom.png"><img alt="Momentum steps zoom on increasing weight update size" src="http://neupy.com/_images/momentum-steps-zoom.png" style="width: 80%;"/></a>
</div>
<p>When we zoom the plot we can see that the direction for weight update vectors is almost the same and gradient’s direction doesn’t change after every epoch. In the picture above the vector which is the last on the right is bigger than the first one on the same plot on the left. Since it always moves forward it speeds up.</p>
<p>Let’s get back to the ball example. What happens when the ball reaches the pit of the half-pipe for the first time? Will it stop? Of course not. Ball gained enough speed for moving. So it will go up. But after that the ball will start to slow down and its amplitude will become smaller and smaller, because of the gravity force, that will continue to push it down to the pit and eventually it will stop to move. Let’s try to find the similar behavior in the same plot.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="http://neupy.com/_images/momentum-steps-zoom-decrease.png"><img alt="Momentum steps zoom on decreasing weight update size" src="http://neupy.com/_images/momentum-steps-zoom-decrease.png" style="width: 80%;"/></a>
</div>
<p>From the figure above it’s clear that weight update magnitude became smaller. Like a ball that slows down and changes its direction towards the minimum.</p>
<p>And finally to make it even more intuitive you can check weight update trajectory in 3D plot. It looks much more like the ball and half-pipe in skatepark analogy.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="http://neupy.com/_images/momentum-3d-trajectory.png"><img alt="Momentum 3D trajectory" src="http://neupy.com/_images/momentum-3d-trajectory.png" style="width: 80%;"/></a>
</div>
</div>
<div class="section" id="rprop">
<h3><a class="toc-backref" href="#id9">RPROP</a></h3>
<p><a class="reference internal" href="http://neupy.com/apidocs/neupy.algorithms.gd.momentum.html#neupy.algorithms.gd.momentum.Momentum" title="neupy.algorithms.gd.momentum.Momentum"><span class="xref py py-class docutils literal"><span class="pre">Momentum</span></span></a> makes fewer steps to reach the specified minimum point, but we still can do better. Next algorithm that we are going to check is <a class="reference internal" href="http://neupy.com/apidocs/neupy.algorithms.gd.rprop.html#neupy.algorithms.gd.rprop.RPROP" title="neupy.algorithms.gd.rprop.RPROP"><span class="xref py py-class docutils literal"><span class="pre">RPROP</span></span></a>.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="http://neupy.com/_images/rprop-steps.png"><img alt="RPROP steps" src="http://neupy.com/_images/rprop-steps.png" style="width: 80%;"/></a>
</div>
<p>This improvement looks impressive. Now we are able to see steps without zooming. We got almost the same value as before using just 20 steps, which is approximately 5 times less than <a class="reference internal" href="http://neupy.com/apidocs/neupy.algorithms.gd.momentum.html#neupy.algorithms.gd.momentum.Momentum" title="neupy.algorithms.gd.momentum.Momentum"><span class="xref py py-class docutils literal"><span class="pre">Momentum</span></span></a> and approximately 40 times less than <a class="reference internal" href="http://neupy.com/apidocs/neupy.algorithms.gd.base.html#neupy.algorithms.gd.base.GradientDescent" title="neupy.algorithms.gd.base.GradientDescent"><span class="xref py py-class docutils literal"><span class="pre">Gradient</span> <span class="pre">Descent</span></span></a>.</p>
<p>Now we are going to figure out what are the main features of <a class="reference internal" href="http://neupy.com/apidocs/neupy.algorithms.gd.rprop.html#neupy.algorithms.gd.rprop.RPROP" title="neupy.algorithms.gd.rprop.RPROP"><span class="xref py py-class docutils literal"><span class="pre">RPROP</span></span></a>. We can notice just by looking at the plot above <a class="reference internal" href="http://neupy.com/apidocs/neupy.algorithms.gd.rprop.html#neupy.algorithms.gd.rprop.RPROP" title="neupy.algorithms.gd.rprop.RPROP"><span class="xref py py-class docutils literal"><span class="pre">RPROP</span></span></a> has a unique step for each weight. There are just two steps for each weight in the input layer for this network. <a class="reference internal" href="http://neupy.com/apidocs/neupy.algorithms.gd.rprop.html#neupy.algorithms.gd.rprop.RPROP" title="neupy.algorithms.gd.rprop.RPROP"><span class="xref py py-class docutils literal"><span class="pre">RPROP</span></span></a> will increase the step size if gradient don’t change the sign compare to previous epoch, and it will decrease otherwise.</p>
<p>Let’s check a few first weight updates.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="http://neupy.com/_images/rprop-first-11-steps.png"><img alt="RPROP first 11 steps" src="http://neupy.com/_images/rprop-first-11-steps.png" style="width: 80%;"/></a>
</div>
<p>From the figure above you can see that first 11 updates have the same direction, so both steps increase their value after each iteration. For the first epoch steps are equal to the same value which we set up at network initialization step. In further iterations they increased by the same constant factor, so after six iteration they got bigger, but they are still equal because they move in one direction all the time.</p>
<p>Now let’s check the next epochs from the figure below. At the 12th epoch gradient changed the direction, but steps are still the same in value. But we can clearly see that gradient changed the sign for the second weight. <a class="reference internal" href="http://neupy.com/apidocs/neupy.algorithms.gd.rprop.html#neupy.algorithms.gd.rprop.RPROP" title="neupy.algorithms.gd.rprop.RPROP"><span class="xref py py-class docutils literal"><span class="pre">RPROP</span></span></a> updated the step after weight had updated, so the step for the second weight should be smaller for the 13th epoch.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="http://neupy.com/_images/rprop-11th-to-14th-epochs.png"><img alt="RPROP from 11th to 14th steps" src="http://neupy.com/_images/rprop-11th-to-14th-epochs.png" style="width: 80%;"/></a>
</div>
<p>Now let’s look at the 13th epoch. It shows us how gradient sign difference at the 12th epoch updated steps. Now the steps are not equal. From the picture above we can see that update on the second weight (y axis) is smaller than on the first weight (x axis).</p>
<p>At the 16th epoch gradient on y axis changed the sign again. Network decreased by constant factor and updated for the second weight at the 17th epoch would be smaller than at the 16th.</p>
<p>To train your intuition you can check the other epochs updates and try to figure out how steps depend on the direction.</p>
</div>
<div class="section" id="irprop">
<h3><a class="toc-backref" href="#id10">iRPROP+</a></h3>
<p><a class="reference internal" href="http://neupy.com/apidocs/neupy.algorithms.gd.rprop.html#neupy.algorithms.gd.rprop.IRPROPPlus" title="neupy.algorithms.gd.rprop.IRPROPPlus"><span class="xref py py-class docutils literal"><span class="pre">iRPROP+</span></span></a> is almost the same algorithm as <a class="reference internal" href="http://neupy.com/apidocs/neupy.algorithms.gd.rprop.html#neupy.algorithms.gd.rprop.RPROP" title="neupy.algorithms.gd.rprop.RPROP"><span class="xref py py-class docutils literal"><span class="pre">RPROP</span></span></a> except a small alteration.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="http://neupy.com/_images/irprop-plus-steps.png"><img alt="iRPROP+ steps" src="http://neupy.com/_images/irprop-plus-steps.png" style="width: 80%;"/></a>
</div>
<p>As in <a class="reference internal" href="http://neupy.com/apidocs/neupy.algorithms.gd.rprop.html#neupy.algorithms.gd.rprop.RPROP" title="neupy.algorithms.gd.rprop.RPROP"><span class="xref py py-class docutils literal"><span class="pre">RPROP</span></span></a> algorithm <a class="reference internal" href="http://neupy.com/apidocs/neupy.algorithms.gd.rprop.html#neupy.algorithms.gd.rprop.IRPROPPlus" title="neupy.algorithms.gd.rprop.IRPROPPlus"><span class="xref py py-class docutils literal"><span class="pre">iRPROP+</span></span></a> make exactly the same first 11 steps.</p>
<p>Now let’s look at the 12th step in the figure below.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="http://neupy.com/_images/irprop-plus-second-part.png"><img alt="iRPROP+ second part" src="http://neupy.com/_images/irprop-plus-second-part.png" style="width: 80%;"/></a>
</div>
<p>Second weight (on the y axis) didn’t change the value. At the same epoch <a class="reference internal" href="http://neupy.com/apidocs/neupy.algorithms.gd.rprop.html#neupy.algorithms.gd.rprop.RPROP" title="neupy.algorithms.gd.rprop.RPROP"><span class="xref py py-class docutils literal"><span class="pre">RPROP</span></span></a> changed the gradient comparing to the previous epoch and just decreased step value after weight update whereas, <a class="reference internal" href="http://neupy.com/apidocs/neupy.algorithms.gd.rprop.html#neupy.algorithms.gd.rprop.IRPROPPlus" title="neupy.algorithms.gd.rprop.IRPROPPlus"><span class="xref py py-class docutils literal"><span class="pre">iRPROP+</span></span></a> disabled weight update for current epoch (set it up to <cite>0</cite>). And of course it also decreased the step for the second weight. Also you can find that vector for the 12th epoch that looks smaller than for the <a class="reference internal" href="http://neupy.com/apidocs/neupy.algorithms.gd.rprop.html#neupy.algorithms.gd.rprop.RPROP" title="neupy.algorithms.gd.rprop.RPROP"><span class="xref py py-class docutils literal"><span class="pre">RPROP</span></span></a> algorithm, because we ignored the second weight update. If we check the x axis update size we will find that it has the same value as in <a class="reference internal" href="http://neupy.com/apidocs/neupy.algorithms.gd.rprop.html#neupy.algorithms.gd.rprop.RPROP" title="neupy.algorithms.gd.rprop.RPROP"><span class="xref py py-class docutils literal"><span class="pre">RPROP</span></span></a> algorithm.</p>
<p>At 13th epoch network again included second weight into the update process, because compared to the previous epoch gradient didn’t change its sign.</p>
<p>The nice thing about this algorithm is that it tries to move in a new direction instead of going back and force and trying to redo updates from the previous epochs.</p>
</div>
<div class="section" id="gradient-descent-and-golden-search">
<h3><a class="toc-backref" href="#id11">Gradient Descent and Golden Search</a></h3>
<p>The last algorithm that I want to show is a <a class="reference internal" href="http://neupy.com/apidocs/neupy.algorithms.step_update.linear_search.html#neupy.algorithms.step_update.linear_search.LinearSearch" title="neupy.algorithms.step_update.linear_search.LinearSearch"><span class="xref py py-class docutils literal"><span class="pre">Golden</span> <span class="pre">Search</span></span></a>. This algorithm is not able to train a network by itself, but it can help other algorithms to do it better. I will use Gradient Descent to show the huge improvement that gives <a class="reference internal" href="http://neupy.com/apidocs/neupy.algorithms.step_update.linear_search.html#neupy.algorithms.step_update.linear_search.LinearSearch" title="neupy.algorithms.step_update.linear_search.LinearSearch"><span class="xref py py-class docutils literal"><span class="pre">Golden</span> <span class="pre">Search</span></span></a>.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="http://neupy.com/_images/grad-descent-and-gold-search-steps.png"><img alt="Gradient Descent with Golden Search steps" src="http://neupy.com/_images/grad-descent-and-gold-search-steps.png" style="width: 80%;"/></a>
</div>
<p>It took just two steps to reach the goal. Let’s check the first step. <a class="reference internal" href="http://neupy.com/apidocs/neupy.algorithms.step_update.linear_search.html#neupy.algorithms.step_update.linear_search.LinearSearch" title="neupy.algorithms.step_update.linear_search.LinearSearch"><span class="xref py py-class docutils literal"><span class="pre">Golden</span> <span class="pre">Search</span></span></a> helps to find the best step size that can be in a specified direction. So basically, it just tries multiple combinations until it finds the best one. As you can see from the plot the first step size is almost perfect for the specified direction. If you went farther you would increase the error.</p>
<p>The main disadvantage of <a class="reference internal" href="http://neupy.com/apidocs/neupy.algorithms.step_update.linear_search.html#neupy.algorithms.step_update.linear_search.LinearSearch" title="neupy.algorithms.step_update.linear_search.LinearSearch"><span class="xref py py-class docutils literal"><span class="pre">Golden</span> <span class="pre">Search</span></span></a> is a time complexity. It will take a while to find a good step in specified direction. So for the more complicated networks it can take a lot of time to find a perfect step size.</p>
</div>
</div>
<div class="section" id="bring-them-all-together">
<h2><a class="toc-backref" href="#id12">Bring them all together</a></h2>
<div class="figure align-center">
<a class="reference internal image-reference" href="http://neupy.com/_images/all-algorithms-steps.png"><img alt="All algorithms steps" src="http://neupy.com/_images/all-algorithms-steps.png" style="width: 80%;"/></a>
</div>
</div>
<div class="section" id="summary">
<h2><a class="toc-backref" href="#id13">Summary</a></h2>
<table border="1" class="docutils" id="id2">
<caption><span class="caption-text">Summary table</span></caption>
<colgroup>
<col width="50%"/>
<col width="50%"/>
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Algorithm</th>
<th class="head">Number of epochs</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>Gradient Descent</td>
<td>797</td>
</tr>
<tr class="row-odd"><td>Momentum</td>
<td>92</td>
</tr>
<tr class="row-even"><td>RPROP</td>
<td>20</td>
</tr>
<tr class="row-odd"><td>iRPROP+</td>
<td>17</td>
</tr>
<tr class="row-even"><td>Gradient Descent + Golden Search</td>
<td>2</td>
</tr>
</tbody>
</table>
<div class="figure align-center">
<a class="reference internal image-reference" href="http://neupy.com/_images/compare-number-of-epochs.png"><img alt="Compare number of epochs" src="http://neupy.com/_images/compare-number-of-epochs.png" style="width: 80%;"/></a>
</div>
<p>There is no perfect algorithm for neural network that can solve all problems. All of them have their own pros and cons. Some of the algorithms can be memory or computationally overwhelming and you have to choose an algorithm depending on the task you want to solve.</p>
<p>All code is available at <a class="reference external" href="https://github.com/itdxer/neupy/blob/master/examples/mlp/gd_algorithms_visualization.py">GitHub</a>. You can play around with the script and set up different learning algorithms and hyperparameters. More algorithms you can find at NeuPy’s <a class="reference internal" href="http://neupy.com/pages/cheatsheet.html#cheat-sheet"><span>Cheat sheet</span></a>.</p>
</div>
]]></description>
             <pubDate>Sat, 04 Jul 2015 00:00:00 +0200</pubDate>
        </item>
    
    </channel>
</rss>